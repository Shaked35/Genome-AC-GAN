{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from utils.util import *\n",
    "%matplotlib inline\n",
    "output_folder = \"classifier_analysis\"\n",
    "hapt_genotypes_path = '../resource/train_0.8_super_pop.csv'\n",
    "experiment_results = '../experiment_results/polyloss_ce_10k_pop'\n",
    "output_results_path = 'catboost_classification_results.csv'\n",
    "target_column = 'Superpopulation code'\n",
    "(x_train, y_train), _, _, class_to_id = init_dataset(hapt_genotypes_path=hapt_genotypes_path,\n",
    "                                                     target_column=target_column, without_extra_data=True)\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "test_dataset = prepare_test_and_fake_dataset(experiment_results, test_path=\"../resource/test_0.2_super_pop.csv\")\n",
    "y_train = np.argmax(y_train, axis=-1)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_samples = prepare_test_and_fake_dataset(experiment_results,\n",
    "                                                  test_path=\"../resource/Genome-AC-GAN By Continental Population genotypes.hapt\",\n",
    "                                                  from_generated=True)\n",
    "generated_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def concatenate_fake_data(percentage, generated_samples, x_train, y_train):\n",
    "    if percentage == 0:\n",
    "        return (x_train, y_train), 0\n",
    "    num_samples = int(percentage * len(generated_samples[0]))\n",
    "    generated_samples_batches = tensorflow.data.Dataset.from_tensor_slices(generated_samples).shuffle(\n",
    "        generated_samples[0].shape[0]).batch(num_samples, drop_remainder=True)\n",
    "    for x_generated_random_batch, Y_generated_random_batch in generated_samples_batches:\n",
    "        train_dataset_with_generated_data = (\n",
    "            np.concatenate((x_train, x_generated_random_batch), axis=0),\n",
    "            np.concatenate((y_train, Y_generated_random_batch), axis=0)\n",
    "        )\n",
    "        return train_dataset_with_generated_data, num_samples\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_models():\n",
    "    nb = MultinomialNB()\n",
    "    knn = KNeighborsClassifier()\n",
    "    lgr = LogisticRegression(multi_class='multinomial', max_iter=100)\n",
    "    return {'NB': nb, 'KNN': knn, 'LGR': lgr}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rows = []\n",
    "scores = []\n",
    "test_predictions = []\n",
    "number_of_iters = 2\n",
    "k = 5\n",
    "model_types = 1\n",
    "number_of_models = 0\n",
    "min_synthetic_percentage = 0\n",
    "max_synthetic_percentage = 1\n",
    "step_synthetic_percentage = 0.05\n",
    "for i in range(model_types):\n",
    "    classifiers = init_models()\n",
    "    start_time = datetime.now()\n",
    "    for index, (model_name, clf) in enumerate(classifiers.items()):\n",
    "        for synthetic_percentage in np.arange(min_synthetic_percentage,\n",
    "                                              max_synthetic_percentage + step_synthetic_percentage,\n",
    "                                              step_synthetic_percentage):\n",
    "            train_dataset_with_generated_data, number_of_synthetic_samples = concatenate_fake_data(\n",
    "                percentage=synthetic_percentage,\n",
    "                generated_samples=generated_samples,\n",
    "                x_train=x_train, y_train=y_train)\n",
    "            clf.fit(train_dataset_with_generated_data[0], train_dataset_with_generated_data[1])\n",
    "            scores.append(cross_val_score(clf, train_dataset_with_generated_data[0],\n",
    "                                          train_dataset_with_generated_data[1],\n",
    "                                          cv=k)[:])\n",
    "            test_predictions = clf.predict(test_dataset[0])\n",
    "            test_score = accuracy_score(test_dataset[1], test_predictions)\n",
    "            test_cohen_kappa = cohen_kappa_score(test_dataset[1], test_predictions)\n",
    "            rows.append(\n",
    "                {\"synthetic_percentage\": synthetic_percentage,\n",
    "                 \"samples_and_percentage\": f\"{number_of_synthetic_samples}\\n{int(synthetic_percentage * 100)}%\",\n",
    "                 \"model_name\": model_name, \"accuracy\": test_score,\n",
    "                 \"cohen_kappa\": test_cohen_kappa})\n",
    "            number_of_models += 1\n",
    "            if number_of_models % 10 == 0:\n",
    "                print(f\"finished train {number_of_models} models\")\n",
    "    end_time = datetime.now()\n",
    "    duration_minutes = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "    print(f\"finished model iteration in  {duration_minutes} minutes\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rows)\n",
    "results.to_csv(os.path.join(\"classifier_analysis\", \"classifiers_results.csv\"))\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_number_of_samples(df):\n",
    "    # Sort the dataframe by number_of_samples in ascending order\n",
    "    sorted_df = df.sort_values('samples_and_percentage')\n",
    "\n",
    "    # Create a figure with two subplots, one for accuracy and the other for Cohen's kappa\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "    # Plot accuracy on the first subplot\n",
    "    sns.pointplot(x='samples_and_percentage', y='accuracy', data=sorted_df, ax=axes[0])\n",
    "    axes[0].set_xlabel('Additional Synthetic Samples')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Accuracy')\n",
    "\n",
    "    # Plot Cohen's kappa on the second subplot\n",
    "    sns.pointplot(x='samples_and_percentage', y='cohen_kappa', data=sorted_df, ax=axes[1])\n",
    "    axes[1].set_xlabel('Additional Synthetic Samples')\n",
    "    axes[1].set_ylabel('Cohen\\'s Kappa')\n",
    "    axes[1].set_title('Cohen\\'s Kappa')\n",
    "\n",
    "    # Adjust the layout of subplots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"classifier_analysis\", 'classifier_with_synthetic_compare.jpg'))\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and return the maximum accuracy for each model\n",
    "    results = df.groupby('model_name')['accuracy'].max().to_dict()\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_number_of_samples(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_p_value(tmp_df, synthetic_percentage_add_compare, target_score=\"accuracy\"):\n",
    "    t_stat, p_value = stats.ttest_ind(tmp_df[tmp_df[\"synthetic_percentage\"] == 0][target_score],\n",
    "                                      tmp_df[tmp_df[\"synthetic_percentage\"].isin(synthetic_percentage_add_compare)][\n",
    "                                          target_score])\n",
    "    m_train = tmp_df[tmp_df[\"synthetic_percentage\"] == 0][target_score].mean()\n",
    "    m_train_s = tmp_df[tmp_df[\"synthetic_percentage\"].isin(synthetic_percentage_add_compare)][target_score].mean()\n",
    "    number_of_models = tmp_df[tmp_df[\"synthetic_percentage\"].isin(synthetic_percentage_add_compare)][\n",
    "        target_score].count()\n",
    "    print(\n",
    "        f\"p_value: {round(1 - p_value, 8)}, mean_train:{m_train}, mean_train_s:{m_train_s}, number_of_models: {number_of_models}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_p_value(results, [0.2, 0.25], \"cohen_kappa\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
