{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizer_v2.rmsprop import RMSprop\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from genome_ac_gan_training import polyloss_ce\n",
    "from utils.util import *\n",
    "\n",
    "init_gpus()\n",
    "\n",
    "output_folder = \"classifier_analysis\"\n",
    "train_set = '../resource/train_AFR_pop.csv'\n",
    "test_set = '../resource/test_AFR_pop.csv'\n",
    "experiment_results = '../resource/sub_AFR_aug_genotypes.hapt'\n",
    "\n",
    "output_file = \"classifiers_results_kmean.csv\"\n",
    "target_column = 'Population code'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(output_folder, output_file))\n",
    "# rows = df.to_dict(orient='records')\n",
    "rows = []\n",
    "len(rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(x_train, y_train), class_id_to_counts, _, class_to_id = init_dataset(hapt_genotypes_path=train_set,\n",
    "                                                                      target_column=target_column,\n",
    "                                                                      with_extra_data=False)\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "test_dataset = prepare_test_and_fake_dataset(experiment_results, test_path=test_set,\n",
    "                                             target_column=target_column,\n",
    "                                             class_to_id=class_to_id)\n",
    "\n",
    "y_train = np.argmax(y_train, axis=-1)\n",
    "uniques, counts = np.unique(y_train, return_counts=True)\n",
    "total_samples = len(y_train)\n",
    "percentages = counts / total_samples * 100\n",
    "class_percentage_dict = dict(zip(uniques, percentages))\n",
    "print(class_percentage_dict)\n",
    "\n",
    "generated_samples = prepare_test_and_fake_dataset(experiment_results,\n",
    "                                                  test_path=experiment_results,\n",
    "                                                  from_generated=True,\n",
    "                                                  class_to_id=class_to_id)\n",
    "print(\"shape: \", generated_samples[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calculate_accuracy(precision, recall):\n",
    "    # Assuming precision and recall are in the range [0, 1]\n",
    "    epsilon = 0.00001\n",
    "    return (precision * recall) / ((precision + recall + epsilon) / 2)\n",
    "\n",
    "\n",
    "def shuffle_test_dataset():\n",
    "    indices = np.arange(test_dataset[0].shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return (\n",
    "        test_dataset[0][indices],\n",
    "        np.array(test_dataset[1])[indices]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_classifier(number_of_genotypes: int, alph: float, lr: float):\n",
    "    nn_classifier = Sequential([\n",
    "        Dense(number_of_genotypes // 2, input_shape=(number_of_genotypes,),\n",
    "              kernel_regularizer=regularizers.l2(0.0001)),\n",
    "        LeakyReLU(alpha=alph),\n",
    "        Dense(number_of_genotypes // 3, input_shape=(number_of_genotypes,),\n",
    "              kernel_regularizer=regularizers.l2(0.0001)),\n",
    "        LeakyReLU(alpha=alph),\n",
    "        Dense(number_of_genotypes // 4, input_shape=(number_of_genotypes,),\n",
    "              kernel_regularizer=regularizers.l2(0.0001)),\n",
    "        LeakyReLU(alpha=alph),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    nn_classifier.compile(optimizer=RMSprop(learning_rate=lr), loss=polyloss_ce, metrics='accuracy')\n",
    "\n",
    "    return nn_classifier\n",
    "\n",
    "\n",
    "def concatenate_fake_data(percentage, generated_samples, x_train, y_train):\n",
    "    if percentage == 0:\n",
    "        train_dataset_with_generated_data = (x_train, y_train)\n",
    "        num_samples = 0\n",
    "    else:\n",
    "        num_samples = int(abs(percentage) * len(x_train))\n",
    "        generated_percentage = min(num_samples / float(generated_samples[0].shape[0]), 0.99)\n",
    "        print(\"generated_percentage\", generated_percentage)\n",
    "        _, X_synthetic, _, Y_synthetic = train_test_split(generated_samples[0],\n",
    "                                                          np.array(generated_samples[1]),\n",
    "                                                          test_size=generated_percentage,\n",
    "                                                          stratify=np.array(generated_samples[1]),\n",
    "                                                          random_state=None)\n",
    "        if percentage > 0:\n",
    "            train_dataset_with_generated_data = (\n",
    "                np.concatenate((x_train, X_synthetic), axis=0),\n",
    "                np.concatenate((y_train, Y_synthetic), axis=0)\n",
    "            )\n",
    "        else:\n",
    "            train_dataset_with_generated_data = (X_synthetic, Y_synthetic)\n",
    "    indices = np.arange(train_dataset_with_generated_data[0].shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Shuffle the dataset using the indices\n",
    "    shuffled_dataset = (\n",
    "        train_dataset_with_generated_data[0][indices],\n",
    "        train_dataset_with_generated_data[1][indices]\n",
    "    )\n",
    "    return shuffled_dataset, num_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def init_models():\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    nn = build_classifier(number_of_genotypes=10000, alph=0.01, lr=0.0001)\n",
    "    svc = SVC()\n",
    "    # {\"lloyd\", \"elkan\", \"auto\", \"full\"}\n",
    "    kmeans = MiniBatchKMeans(n_clusters=7, n_init=3,random_state=42, batch_size=256, max_iter=50, max_no_improvement=30)\n",
    "\n",
    "    # return {'SVC': svc, 'KNN': knn, 'NN': nn,}\n",
    "    return {'KMeans': kmeans}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed\n",
    "\n",
    "scores = []\n",
    "test_predictions = []\n",
    "unique_models = 50\n",
    "number_of_models = 0\n",
    "seed(42)\n",
    "rows_k = []\n",
    "for i in range(unique_models):\n",
    "    for synthetic_percentage in [0, 1]:\n",
    "        classifiers = init_models()\n",
    "        start_time = datetime.now()\n",
    "        for index, (model_name, clf) in enumerate(classifiers.items()):\n",
    "            test_dataset_shuffled = shuffle_test_dataset()\n",
    "\n",
    "            train_dataset_with_generated_data, number_of_synthetic_samples = concatenate_fake_data(\n",
    "                percentage=synthetic_percentage,\n",
    "                generated_samples=generated_samples,\n",
    "                x_train=x_train, y_train=y_train)\n",
    "            print(\n",
    "                f\"{i}: starting model {model_name} with percentage: {synthetic_percentage} and {train_dataset_with_generated_data[0].shape[0]} samples\")\n",
    "\n",
    "            if model_name == 'NN':\n",
    "                Y_train_encoded = tensorflow.one_hot(train_dataset_with_generated_data[1],\n",
    "                                                     depth=train_dataset_with_generated_data[1].max() + 1)\n",
    "                # Split the training dataset into training and validation sets\n",
    "                x_train_tmp, x_val_tmp, y_train_tmp, y_val_tmp = train_test_split(train_dataset_with_generated_data[0],\n",
    "                                                                                  np.array(Y_train_encoded),\n",
    "                                                                                  stratify=np.array(Y_train_encoded),\n",
    "                                                                                  random_state=42,\n",
    "                                                                                  test_size=0.2)\n",
    "\n",
    "                # Define early stopping\n",
    "                early_stopping = EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy',\n",
    "                                               verbose=2)\n",
    "\n",
    "                clf.fit(x_train_tmp, y_train_tmp, batch_size=512, epochs=100, verbose=2,\n",
    "                        validation_data=(x_val_tmp, y_val_tmp), callbacks=[early_stopping])\n",
    "\n",
    "                # Make predictions on the test dataset\n",
    "                test_predictions = tensorflow.argmax(clf.predict(test_dataset_shuffled[0]), axis=1)\n",
    "            elif model_name == 'KMeans':\n",
    "                if synthetic_percentage == 0:\n",
    "                    clf.fit(train_dataset_with_generated_data[0])\n",
    "                else:\n",
    "                    clf.fit(generated_samples[0])\n",
    "                # y_kmeans = clf.labels_\n",
    "                # ari_model1 = adjusted_rand_score(train_dataset_with_generated_data[1], y_kmeans)\n",
    "                # nmi_model1 = normalized_mutual_info_score(train_dataset_with_generated_data[1], y_kmeans)\n",
    "                test_predictions = clf.fit_predict(test_dataset_shuffled[0])\n",
    "                ari_model1 = adjusted_rand_score(test_dataset_shuffled[1], test_predictions)\n",
    "                nmi_model1 = normalized_mutual_info_score(test_dataset_shuffled[1], test_predictions)\n",
    "                rows_k.append({'synthetic_percentage': synthetic_percentage, 'ari_model': round(ari_model1, 5), 'nmi_model': round(nmi_model1, 5)})\n",
    "                print(f\"{synthetic_percentage}: {round(ari_model1, 5)}, {round(nmi_model1, 5)}\")\n",
    "            else:\n",
    "                clf.fit(train_dataset_with_generated_data[0], train_dataset_with_generated_data[1])\n",
    "                test_predictions = clf.predict(test_dataset_shuffled[0])\n",
    "                print(test_predictions)\n",
    "\n",
    "            class_report = classification_report(test_dataset_shuffled[1], test_predictions, output_dict=True)\n",
    "\n",
    "            class_accuracy = {}\n",
    "            for class_label, metrics in class_report.items():\n",
    "                if class_label in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                    continue\n",
    "                class_accuracy[class_label] = round(calculate_accuracy(metrics['precision'], metrics['recall']), 4)\n",
    "\n",
    "            # Print accuracy by class\n",
    "            output_row = {}\n",
    "            for class_label, accuracy in class_accuracy.items():\n",
    "                output_row[id_to_class[int(class_label)]] = accuracy\n",
    "                # print(f\"Class {id_to_class[int(class_label)]}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "            # print(\n",
    "            #     f\"=======> {model_name} {synthetic_percentage}: f1_macro: {class_report['weighted avg']['f1-score']}, f1_weighted, {class_report['macro avg']['f1-score']}, accuracy score: {class_report['accuracy']} on synthetic_percentage: \")\n",
    "\n",
    "            output_row.update({\"synthetic_percentage\": synthetic_percentage,\n",
    "                               \"samples_and_percentage\": f\"{number_of_synthetic_samples}\\n{int(synthetic_percentage * 100)}%\",\n",
    "                               \"model_name\": model_name, \"accuracy\": class_report['accuracy'],\n",
    "                               \"f1_score\": class_report['macro avg']['f1-score']})\n",
    "            rows.append(output_row)\n",
    "            number_of_models += 1\n",
    "            if number_of_models % 10 == 0:\n",
    "                print(f\"finished train {number_of_models} models\")\n",
    "                pd.DataFrame(rows).to_csv(os.path.join(\"classifier_analysis\", output_file))\n",
    "    end_time = datetime.now()\n",
    "    duration_minutes = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "    print(f\"finished model iteration in {duration_minutes} minutes\")\n",
    "\n",
    "pd.DataFrame(rows).to_csv(os.path.join(\"classifier_analysis\", output_file))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install xgboost==2.0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_k = pd.DataFrame(rows_k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"******* median *******\")\n",
    "print(df_k.groupby('synthetic_percentage')[\"ari_model\"].median())\n",
    "\n",
    "print(\"\\n******* mean *******\")\n",
    "print(df_k.groupby('synthetic_percentage')[\"nmi_model\"].median())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
