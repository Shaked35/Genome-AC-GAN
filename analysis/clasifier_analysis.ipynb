{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import json\n",
    "\n",
    "from tensorflow.python.keras.saving.save import load_model\n",
    "\n",
    "from gs_ac_gan_training import f1_score_with_penalty\n",
    "from utils.util import *\n",
    "\n",
    "discriminator_folder = \"../experiment_results/discriminator_0.8_test\"\n",
    "test_path = \"../resource/test_0.2_super_pop.csv\"\n",
    "target_column = \"Superpopulation code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discriminator = load_model(\n",
    "    os.path.join(discriminator_folder, \"discriminator\"),\n",
    "    custom_objects={'f1_score_with_penalty': f1_score_with_penalty})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init class ids as model trained\n",
    "with open(os.path.join(discriminator_folder, \"class_id_map.json\"), 'r') as file:\n",
    "    json_data = file.read()\n",
    "\n",
    "class_to_id = json.loads(json_data)\n",
    "class_to_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(test_path)\n",
    "relevant_columns = get_relevant_columns(test_set, [SAMPLE_COLUMN_NAME, target_column])\n",
    "test_set = filter_samples_by_minimum_examples(10, test_set, target_column)\n",
    "test_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, _, y_real = extract_y_column(class_to_id, test_set, target_column)\n",
    "y_real = tensorflow.argmax(y_real, axis=1)\n",
    "x_values = extract_x_values(test_set, relevant_columns, target_column)\n",
    "x_values = x_values - np.random.uniform(0, 0.1, size=(x_values.shape[0], x_values.shape[1]))\n",
    "x_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, class_predictions = discriminator.predict_on_batch([x_values])\n",
    "y_pred = tensorflow.argmax(class_predictions, axis=1)\n",
    "uniques, counts = np.unique(y_pred, return_counts=True)\n",
    "class_id_to_counts = dict(zip(uniques, counts))\n",
    "print(class_id_to_counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = list(class_to_id.values())\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_real, y_pred):\n",
    "    # create a confusion matrix using numpy\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    for i in range(len(y_real)):\n",
    "        cm[int(y_real[i]), int(y_pred[i])] += 1\n",
    "\n",
    "    # plot a heat map of the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            _ = ax.text(j, i, int(cm[i, j]),\n",
    "                        ha='center', va='center', color='w')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_bar_comparison(y_real, y_pred):\n",
    "    # plot a bar chart of the number of true and predicted labels side by side\n",
    "    fig, ax = plt.subplots()\n",
    "    counts_real = np.zeros(len(labels))\n",
    "    counts_pred = np.zeros(len(labels))\n",
    "    width = 0.4\n",
    "    positions = np.arange(len(labels))\n",
    "    for i in range(len(y_real)):\n",
    "        counts_real[int(y_real[i])] += 1\n",
    "        counts_pred[int(y_pred[i])] += 1\n",
    "    ax.bar(positions - width / 2, counts_real, width=width, color='blue', label='True label')\n",
    "    ax.bar(positions + width / 2, counts_pred, width=width, color='orange', label='Predicted label')\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Number of samples')\n",
    "    plt.title('True vs. predicted label distribution')\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    # add the numbers on each bar\n",
    "    for i in range(len(positions)):\n",
    "        ax.text(positions[i] - width / 2, counts_real[i] + 0.1, str(int(counts_real[i])), ha='center')\n",
    "        ax.text(positions[i] + width / 2, counts_pred[i] + 0.1, str(int(counts_pred[i])), ha='center')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_real, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_bar_comparison(y_real, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_dataframe_correlation(df):\n",
    "    # calculate correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation between variables')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_dataframe_correlation(\n",
    "    pd.concat([pd.DataFrame(y_real, columns=['y_real']), pd.DataFrame(y_pred, columns=['y_pred'])], axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_correlation(y_real, y_pred):\n",
    "    # plot a scatter plot of predicted vs true labels with a trendline\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_real, y_pred)\n",
    "    m, b = np.polyfit(y_real, y_pred, 1)\n",
    "    ax.plot(y_real, m * y_real + b, color='red')\n",
    "    ax.set_xlabel('True label')\n",
    "    ax.set_ylabel('Predicted label')\n",
    "    plt.title('Predicted vs true label correlation')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation(y_real, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_histogram(y_real, y_pred):\n",
    "    # plot a histogram of the true and predicted labels\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(y_real, bins=len(labels), alpha=0.5, label='True label')\n",
    "    ax.hist(y_pred, bins=len(labels), alpha=0.5, label='Predicted label')\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('True vs. predicted label histogram')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_histogram(y_real, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_box(y_real, y_pred):\n",
    "    # plot a box plot of the true and predicted labels\n",
    "    fig, ax = plt.subplots()\n",
    "    data = [y_real, y_pred]\n",
    "    ax.boxplot(data, labels=['True label', 'Predicted label'])\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_title('True vs. predicted label box plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_box(y_real, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def plot_accuracy(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar([\"Accuracy\"], [acc])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Model Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(y_real, y_pred)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
