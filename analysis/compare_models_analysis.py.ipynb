{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from utils.util import *\n",
    "%matplotlib inline\n",
    "try:\n",
    "    import ot\n",
    "\n",
    "    ot_loaded = True\n",
    "except ModuleNotFoundError:\n",
    "    ot_loaded = False\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    sm_loaded = True\n",
    "except ModuleNotFoundError:\n",
    "    sm_loaded = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [],
   "source": [
    "models_to_data = {\n",
    "    \"Real\": {\n",
    "        \"path\": \"resource/test_0.2_super_pop.csv\",\n",
    "        \"color\": \"black\",\n",
    "        \"type\": \"dataset\"\n",
    "    },\n",
    "    \"Train Set\": {\n",
    "        \"path\": \"resource/train_0.8_super_pop.csv\",\n",
    "        \"color\": \"gray\",\n",
    "        \"type\": \"dataset\"\n",
    "    },\n",
    "    \"RBM 2023\": {\n",
    "        \"path\": \"fake_genotypes_sequences/preview_sequences/10K_SNP_GAN_AG_10800Epochs.hapt\",\n",
    "        \"color\": \"yellow\"\n",
    "    },\n",
    "    \"WGAN 2023\": {\n",
    "        \"path\": \"fake_genotypes_sequences/preview_sequences/10K_WGAN.hapt\",\n",
    "        \"color\": \"purple\"\n",
    "    },\n",
    "    \"GAN 2019 Retrain\": {\n",
    "        \"path\": \"fake_genotypes_sequences/preview_sequences/old GAN retrain genotypes.hapt\",\n",
    "        \"color\": \"brown\",\n",
    "        \"type\": \"retrain_old_model\"\n",
    "    },\n",
    "    \"Genome-AC-GAN By National Population\": {\n",
    "        \"path\": \"resource/Genome-AC-GAN By National Population genotypes.hapt\",\n",
    "        \"color\": \"green\",\n",
    "        \"type\": \"new_model\"\n",
    "    },\n",
    "    \"Genome-AC-GAN By Continental Population\": {\n",
    "        \"path\": \"resource/Genome-AC-GAN By Continental Population genotypes.hapt\",\n",
    "        \"color\": \"blue\",\n",
    "        \"type\": \"new_model\"\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "output_dir = os.environ.get(\"output_dir\", DEFAULT_EXPERIMENT_OUTPUT_DIR)\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "compute_AATS = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [],
   "source": [
    "color_palette = {model_name: values[\"color\"] for (model_name, values) in models_to_data.items()}\n",
    "sns.set_palette(color_palette.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "def load_analysis_data_agg_tests(models_to_data: dict, number_of_datasets: int):\n",
    "    transformations = {'to_minor_encoding': False, 'min_af': 0, 'max_af': 1}\n",
    "\n",
    "    model_keep_all_snps, sample_info = dict(), dict()\n",
    "    # initialize real data\n",
    "    real_data = models_to_data['Real']\n",
    "    real_model_sequences, number_of_samples = create_single_dataset(\n",
    "        real_data, f\"../{real_data['path']}\", 'Real', 0,\n",
    "        sample_info)\n",
    "    datasets = {'Real': [np.array(real_model_sequences.loc[:, 2:].astype(int))]}\n",
    "    full_datasets = {'Real': np.array(real_model_sequences.loc[:, 2:].astype(int))}\n",
    "    print('Real: ', datasets['Real'][0].shape)\n",
    "    # init all other datasets\n",
    "    for model_name, data in models_to_data.items():\n",
    "        if model_name != 'Real':\n",
    "            print(f\"init data from {model_name} with type {data.get('type', 'none')}\")\n",
    "            model_datasets = []\n",
    "            file_path = f\"../{data['path']}\"\n",
    "            for dataset_number in range(1, number_of_datasets + 1):\n",
    "\n",
    "                model_sequences, _ = create_single_dataset(data, file_path, model_name,\n",
    "                                                           number_of_samples,\n",
    "                                                           sample_info)\n",
    "                model_datasets.append(np.array(model_sequences.loc[:, 2:].astype(int)))\n",
    "\n",
    "                if dataset_number % 5 == 0:\n",
    "                    print(f\"Finished init model {model_name} number {dataset_number}\")\n",
    "            datasets[model_name] = model_datasets\n",
    "\n",
    "            model_sequences, _ = create_single_dataset(data, file_path, model_name,\n",
    "                                                       number_of_samples,\n",
    "                                                       sample_info, filter_number_of_sequences=False)\n",
    "            full_datasets[model_name] = np.array(model_sequences.loc[:, 2:].astype(int))\n",
    "\n",
    "    extra_sample_info = pd.DataFrame(np.concatenate(list(sample_info.values())), columns=['label', 'id'])\n",
    "    print(\"Dictionary of datasets:\", len(datasets))\n",
    "    return extra_sample_info, sample_info, datasets, transformations, model_keep_all_snps, number_of_samples, full_datasets\n",
    "\n",
    "\n",
    "def create_single_dataset(data, file_path, model_name, number_of_samples, sample_info, filter_number_of_sequences=True):\n",
    "    if data.get(\"type\", \"\") == \"dataset\":\n",
    "        model_sequences_df = pd.read_csv(file_path)\n",
    "        columns = get_relevant_columns(model_sequences_df, model_sequences_df.columns[:2])\n",
    "        model_sequences = model_sequences_df[columns]\n",
    "        columns = [int(i) for i in columns]\n",
    "        model_sequences.columns = columns\n",
    "        model_sequences = model_sequences.sample(frac=1).reset_index(drop=True)\n",
    "    else:\n",
    "        model_sequences = pd.read_csv(file_path, sep=' ', header=None)\n",
    "        if data.get(\"type\", \"\") == \"new_model\":\n",
    "            model_sequences.columns = [column if column == 0 else column + 1 for column in model_sequences.columns]\n",
    "            # Calculate the category counts\n",
    "            if filter_number_of_sequences:\n",
    "                category_counts = model_sequences[0].value_counts()\n",
    "                sample_counts = (category_counts / category_counts.sum() * (number_of_samples)).astype(int)\n",
    "                model_sequences = model_sequences.sample(frac=1).reset_index(drop=True)\n",
    "                # Sample rows from each category\n",
    "                model_sequences = model_sequences.groupby(0).apply(\n",
    "                    lambda x: x.sample(sample_counts[x.name])).reset_index(drop=True)\n",
    "\n",
    "            model_sequences.insert(0, 1, [f\"AG{sample_id}\" for sample_id in range(model_sequences.shape[0])])\n",
    "        if data.get(\"type\", \"\") == \"retrain_old_model\":\n",
    "            model_sequences = model_sequences.drop(columns=list(model_sequences.columns)[-1], axis=1)\n",
    "            model_sequences.columns = [column + 2 for column in list(model_sequences.columns)]\n",
    "            model_sequences.insert(loc=0, column=0, value=\"none\")\n",
    "            model_sequences.insert(loc=1, column=1, value='none')\n",
    "    if model_name == 'Real':\n",
    "        number_of_samples = len(model_sequences)\n",
    "\n",
    "    if filter_number_of_sequences:\n",
    "        if model_sequences.shape[0] > number_of_samples:\n",
    "            model_sequences = model_sequences.drop(\n",
    "                index=np.sort(\n",
    "                    np.random.choice(np.arange(model_sequences.shape[0]),\n",
    "                                     size=model_sequences.shape[0] - number_of_samples,\n",
    "                                     replace=False)))\n",
    "    # overwrite file first column to set the label name chosen in infiles (eg GAN, etc):\n",
    "    model_sequences[0] = model_name\n",
    "    sample_info[model_name] = pd.DataFrame({'label': model_sequences[0], 'ind': model_sequences[1]})\n",
    "    return model_sequences, number_of_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:  (1002, 10000)\n",
      "init data from Train Set with type dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[305], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m extra_sample_info, sample_info, multiple_datasets, transformations, model_keep_all_snps, number_of_samples, full_datasets \u001B[38;5;241m=\u001B[39m \u001B[43mload_analysis_data_agg_tests\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodels_to_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[304], line 21\u001B[0m, in \u001B[0;36mload_analysis_data_agg_tests\u001B[0;34m(models_to_data, number_of_datasets)\u001B[0m\n\u001B[1;32m     18\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset_number \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, number_of_datasets \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 21\u001B[0m     model_sequences, _ \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_single_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mnumber_of_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43msample_info\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     model_datasets\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(model_sequences\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;241m2\u001B[39m:]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)))\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset_number \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[304], line 42\u001B[0m, in \u001B[0;36mcreate_single_dataset\u001B[0;34m(data, file_path, model_name, number_of_samples, sample_info, filter_number_of_sequences)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_single_dataset\u001B[39m(data, file_path, model_name, number_of_samples, sample_info, filter_number_of_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 42\u001B[0m         model_sequences_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m         columns \u001B[38;5;241m=\u001B[39m get_relevant_columns(model_sequences_df, model_sequences_df\u001B[38;5;241m.\u001B[39mcolumns[:\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     44\u001B[0m         model_sequences \u001B[38;5;241m=\u001B[39m model_sequences_df[columns]\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/io/parsers/readers.py:583\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1697\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1699\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1700\u001B[0m     (\n\u001B[1;32m   1701\u001B[0m         index,\n\u001B[1;32m   1702\u001B[0m         columns,\n\u001B[1;32m   1703\u001B[0m         col_dict,\n\u001B[0;32m-> 1704\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1705\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1707\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:812\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:889\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1034\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1088\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1163\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow_27/lib/python3.9/site-packages/pandas/core/dtypes/common.py:1335\u001B[0m, in \u001B[0;36mis_extension_array_dtype\u001B[0;34m(arr_or_dtype)\u001B[0m\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001B[39;00m\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;66;03m#  here too.\u001B[39;00m\n\u001B[1;32m   1328\u001B[0m     \u001B[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001B[39;00m\n\u001B[1;32m   1329\u001B[0m     \u001B[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001B[39;00m\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m   1331\u001B[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001B[1;32m   1332\u001B[0m     )\n\u001B[0;32m-> 1335\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_extension_array_dtype\u001B[39m(arr_or_dtype) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;124;03m    Check if an object is a pandas extension array type.\u001B[39;00m\n\u001B[1;32m   1338\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m   1379\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1380\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(arr_or_dtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, arr_or_dtype)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "extra_sample_info, sample_info, multiple_datasets, transformations, model_keep_all_snps, number_of_samples, full_datasets = load_analysis_data_agg_tests(\n",
    "    models_to_data, 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def plot_pca_comparison(models):\n",
    "    model_to_wasserstein_dists = {}\n",
    "    all_best_sequences = {}\n",
    "    # Extract the 'Real' model data\n",
    "    real_model = models['Real'][0]\n",
    "    all_best_sequences['Real'] = real_model\n",
    "    # Perform PCA on the 'Real' model\n",
    "    pca_real = PCA(n_components=2)\n",
    "    pca_real.fit(real_model)\n",
    "    pca_real_transformed = pca_real.transform(real_model)\n",
    "\n",
    "    # Plotting parameters\n",
    "    num_models = len(models) - 1\n",
    "    num_rows = int(np.ceil(num_models / 3))\n",
    "    num_cols = min(num_models, 3)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "\n",
    "    for i, (model_name, model_sequences) in enumerate(models.items()):\n",
    "        # Skip 'Real' model\n",
    "        if model_name == 'Real':\n",
    "            continue\n",
    "\n",
    "        print(f\"start model: {model_name} get best sequences\")\n",
    "        all_wasserstein_dist, best_model_sequence, best_pca_transformed = \\\n",
    "            get_best_pca_wasserstein(model_sequences, pca_real_transformed)\n",
    "        mean_all_wasserstein_dist = np.mean(all_wasserstein_dist)\n",
    "        std_all_wasserstein_dist = np.std(all_wasserstein_dist)\n",
    "        min_all_wasserstein_dist = np.min(all_wasserstein_dist)\n",
    "        model_to_wasserstein_dists[model_name] = all_wasserstein_dist\n",
    "        all_best_sequences[model_name] = best_model_sequence\n",
    "        print(\n",
    "            f\"finished model: {model_name} get best sequences with mean: {mean_all_wasserstein_dist}, std: {std_all_wasserstein_dist}, min: {min_all_wasserstein_dist}\")\n",
    "        # Set subplot position\n",
    "        position = i - 1\n",
    "        row = position // num_cols\n",
    "        col = position % num_cols\n",
    "\n",
    "        # Plot PCA comparison\n",
    "        ax = axes[row, col]\n",
    "        ax.scatter(pca_real_transformed[:, 0], pca_real_transformed[:, 1], color=color_palette['Real'], alpha=0.8)\n",
    "        ax.scatter(best_pca_transformed[:, 0], best_pca_transformed[:, 1], color=color_palette[model_name], alpha=0.6)\n",
    "        title = \"\\n\".join(model_name.split(\"By\"))\n",
    "        ax.set_title(title, fontsize=25, fontweight='bold')\n",
    "\n",
    "    # Adjust the spacing between the first row and the second column\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, \"pca2_on_test_real.jpg\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    return model_to_wasserstein_dists, all_best_sequences\n",
    "\n",
    "\n",
    "def get_best_pca_wasserstein(model_sequences, pca_real_transformed):\n",
    "    all_wasserstein_dist = []\n",
    "    best_wasserstein_dist = np.inf\n",
    "    best_model_sequence = None\n",
    "    best_pca_transformed = None\n",
    "    for model_sequence in model_sequences:\n",
    "        # Perform PCA on the current model\n",
    "        pca_model = PCA(n_components=2)\n",
    "        pca_model.fit(model_sequence)\n",
    "        pca_model_transformed = pca_model.transform(model_sequence)\n",
    "\n",
    "        # Calculate Wasserstein distance\n",
    "        tmp_wasserstein_dist = wasserstein_distance(pca_real_transformed.flatten(),\n",
    "                                                    pca_model_transformed.flatten())\n",
    "        all_wasserstein_dist.append(tmp_wasserstein_dist)\n",
    "        if tmp_wasserstein_dist < best_wasserstein_dist:\n",
    "            best_wasserstein_dist = tmp_wasserstein_dist\n",
    "            best_model_sequence = model_sequence\n",
    "            best_pca_transformed = pca_model_transformed\n",
    "    return all_wasserstein_dist, best_model_sequence, best_pca_transformed\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_to_wasserstein_dists, all_best_sequences = plot_pca_comparison(multiple_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = list(model_to_wasserstein_dists.keys())\n",
    "model_names = [\"\\n\".join(model_name.split(\"By\")) for model_name in model_names]\n",
    "model_names = [\"\\n\".join(model_name.split(\"Model\")) for model_name in model_names]\n",
    "wasserstein_distances = list(model_to_wasserstein_dists.values())\n",
    "\n",
    "p_values = []\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        p_values.append(1 - stats.ttest_ind(wasserstein_distances[i], wasserstein_distances[j]).pvalue)\n",
    "\n",
    "# Reshape the p_values into a 2D matrix\n",
    "n = len(model_names)\n",
    "p_values_matrix = np.zeros((n, n))\n",
    "p_values_matrix[np.triu_indices(n, 1)] = p_values\n",
    "p_values_matrix += p_values_matrix.T\n",
    "\n",
    "# Create a plot matrix of the p-values\n",
    "fig, ax = plt.subplots(figsize=(15, 15))  # Increase the size of the plot\n",
    "im = ax.imshow(p_values_matrix, cmap='coolwarm', vmin=0, vmax=1)\n",
    "ax.set_xticks(np.arange(len(model_names)))\n",
    "ax.set_yticks(np.arange(len(model_names)))\n",
    "ax.set_xticklabels(model_names, rotation=45)\n",
    "ax.set_yticklabels(model_names)\n",
    "\n",
    "# Add numerical values in the matrix\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        text = ax.text(j, i, f'{p_values_matrix[i, j] * 100:.5f}%', ha='center', va='center', color='w', fontsize=15)\n",
    "\n",
    "plt.colorbar(im)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"P-values wasserstein_distances\"))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wasserstein Distance Plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a dictionary called `wasserstein_dict` with model names as keys and Wasserstein Distance lists as values\n",
    "# and a dictionary called `color_palette` with model names as keys and color names as values\n",
    "\n",
    "# Calculate the mean and standard deviation for each model's Wasserstein Distance list\n",
    "means = {}\n",
    "stds = {}\n",
    "boxprops = dict(linewidth=3, color='black')\n",
    "medianprops = dict(linewidth=3, color='black')\n",
    "meanprops = dict(linewidth=3, color='black')\n",
    "\n",
    "for model, distances in model_to_wasserstein_dists.items():\n",
    "    model_display_name = \"\\n\".join(model.split(\"By\"))\n",
    "    model_display_name = \"\\n\".join(model_display_name.split(\"Model\"))\n",
    "    means[model_display_name] = np.mean(distances)\n",
    "    stds[model_display_name] = np.std(distances)\n",
    "\n",
    "# Sort the model names alphabetically\n",
    "sorted_models = list(model_to_wasserstein_dists.keys())\n",
    "\n",
    "# Get the colors from the color palette based on the sorted model names\n",
    "colors = [color_palette[model] for model in sorted_models]\n",
    "\n",
    "# Plotting the mean and standard deviation for each model\n",
    "data = [model_to_wasserstein_dists[model] for model in sorted_models]\n",
    "sorted_models = [\"\\n\".join(model_name.split(\"By\")) for model_name in sorted_models]\n",
    "sorted_models = [\"\\n\".join(model_name.split(\"Model\")) for model_name in sorted_models]\n",
    "# Plotting the boxplot for each model\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "boxplot = ax.boxplot(data, labels=sorted_models, patch_artist=True, showfliers=False, boxprops=boxprops,\n",
    "                     medianprops=medianprops, meanprops=meanprops)\n",
    "\n",
    "# Set the colors for the boxes based on the color palette\n",
    "for patch_artist, color in zip(boxplot['boxes'], colors):\n",
    "    patch_artist.set_facecolor(color)\n",
    "\n",
    "# Add text annotations for mean and standard deviation values in the label\n",
    "for i, model in enumerate(sorted_models):\n",
    "    mean = means[model]\n",
    "    std = stds[model]\n",
    "    label = f\"Mean: {mean:.2f}\\nStd: {std:.2f}\"\n",
    "    pos = 0.1 if i != 3 else -0.1\n",
    "    ax.text(i + 1, mean + std + pos, label, ha='center', va='top', fontsize=24, color='white', fontweight='bold',\n",
    "            bbox=dict(facecolor='black', edgecolor='black', boxstyle='round', pad=0.2))\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_weight('bold')\n",
    "    label.set_size(16)\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_weight('bold')\n",
    "    label.set_size(16)\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Wasserstein Distance', fontweight='bold', fontsize=17)\n",
    "\n",
    "ax.grid(True, color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(os.path.join(output_dir, \"Wasserstein Distance Comparison.jpg\"), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MAF Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum_alleles_by_position, allele_frequency, is_fixed = build_allele_frequency(full_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def plotreg(x, y, keys, statname, col, model_name_display, ax=None):\n",
    "    \"\"\"\n",
    "    Plot for x versus y with regression scores and returns correlation coefficient and MSE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like, scalar\n",
    "    y : array-like, scalar\n",
    "    keys : tuple\n",
    "        Tuple containing the model names or keys\n",
    "    statname : str\n",
    "        'Allele frequency', 'LD', or '3 point correlation', etc.\n",
    "    col : str\n",
    "        Color code or name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "        Pearson correlation coefficient between x and y\n",
    "    mse : float\n",
    "        Mean Squared Error between x and y\n",
    "    \"\"\"\n",
    "\n",
    "    lims = [np.min(x), np.max(x)]\n",
    "    r, _ = pearsonr(x, y)\n",
    "    mae = mean_absolute_error(x, y)\n",
    "\n",
    "    if sm_loaded:\n",
    "        reg = sm.OLS(x, y).fit()\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    if len(x) < 100:\n",
    "        alpha = 1\n",
    "    else:\n",
    "        alpha = .6\n",
    "\n",
    "    ax.plot(x, y, c=col, marker='o', lw=0, alpha=alpha)\n",
    "    ax.plot(lims, lims, ls='--', alpha=1, c='black')\n",
    "    title = ax.set_title(\n",
    "        f'{model_name_display}\\nCorrelation={round(round(r, 3) * 100, 3)}%\\nMAE={round(round(mae, 3) * 100, 3)}',\n",
    "        fontsize=29, fontweight=\"bold\", y=1, color='black')\n",
    "\n",
    "    title.set_bbox({'facecolor': 'white', 'edgecolor': \"black\", 'pad': 1.2})\n",
    "    ax.set_xlabel(\"MAF In Real\", fontsize=28, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"MAF In Synthetic\", fontsize=28, fontweight=\"bold\")\n",
    "\n",
    "    # Adjust vertical spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    return r, mae\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_allele_frequency(allele_frequency, file_name, maf, highest=False):\n",
    "    # Plotting Allele frequencies in Generated vs Real\n",
    "    # below a certain real frequency\n",
    "    figwi = 14\n",
    "    l, c = 1, 6\n",
    "    plt.figure(figsize=(44, 6))\n",
    "    if highest:\n",
    "        maf = 1 - maf\n",
    "        keep = (allele_frequency['Real'] >= maf)\n",
    "    else:\n",
    "        keep = (allele_frequency['Real'] <= maf)\n",
    "    for i, (model_name, val) in enumerate(allele_frequency.items()):\n",
    "        model_name_display = model_name.replace(\"Population\", \"\").replace(\" By\", \"\")\n",
    "        if model_name != 'Real':\n",
    "            ax = plt.subplot(int(l), c, i)\n",
    "            plotreg(x=allele_frequency['Real'][keep], y=val[keep],\n",
    "                     keys=['Real', model_name_display], statname=\"Allele frequency\",\n",
    "                     col=color_palette[model_name], model_name_display=model_name_display, ax=ax)\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, file_name), bbox_inches='tight', dpi=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_allele_frequency(allele_frequency, 'total_allele_frequency.jpg', 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_allele_frequency(allele_frequency, 'zoom_lowest_total_allele_frequency.jpg', 0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_allele_frequency(allele_frequency, 'zoom_highest_total_allele_frequency.jpg', 0.2, highest=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialized Data With Preview Loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_analysis_data_for_preview_tests(model_name_to_input_file: dict):\n",
    "    transformations = {'to_minor_encoding': False, 'min_af': 0, 'max_af': 1}\n",
    "\n",
    "    datasets, model_keep_all_snps, sample_info = dict(), dict(), dict()\n",
    "    number_of_samples = 0\n",
    "    for model_name, data in model_name_to_input_file.items():\n",
    "        file_path = f\"../{data['path']}\"\n",
    "        print(model_name, \"loaded from\", file_path)\n",
    "        if file_path.endswith('.csv'):\n",
    "            model_sequences = pd.read_csv(file_path)\n",
    "            columns = get_relevant_columns(model_sequences, model_sequences.columns[:2])\n",
    "            model_sequences = model_sequences[columns]\n",
    "            columns = [int(i) for i in columns]\n",
    "            model_sequences.columns = columns\n",
    "            number_of_samples = len(model_sequences)\n",
    "\n",
    "        else:\n",
    "            model_sequences = pd.read_csv(file_path, sep=' ', header=None)\n",
    "            if 'Genome-AC-GAN' in model_name:\n",
    "                model_sequences.columns = [column if column == 0 else column + 1 for column in model_sequences.columns]\n",
    "                model_sequences.insert(0, 1, [f\"AG{sample_id}\" for sample_id in range(model_sequences.shape[0])])\n",
    "            if model_sequences.shape[1] == 808:  # special case for a specific file that had an extra empty column\n",
    "                model_sequences = model_sequences.drop(columns=model_sequences.columns[-1])\n",
    "            if model_sequences.shape[0] > number_of_samples:\n",
    "                model_sequences = model_sequences.drop(\n",
    "                    index=np.sort(\n",
    "                        np.random.choice(np.arange(model_sequences.shape[0]),\n",
    "                                         size=model_sequences.shape[0] - number_of_samples,\n",
    "                                         replace=False))\n",
    "                )\n",
    "            if 'GAN 2019 Retrain' in model_name:\n",
    "                model_sequences = model_sequences.drop(columns=list(model_sequences.columns)[-1], axis=1)\n",
    "                model_sequences.columns = [column + 2 for column in list(model_sequences.columns)]\n",
    "                model_sequences.insert(loc=0, column=0, value=\"none\")\n",
    "                model_sequences.insert(loc=1, column=1, value='none')\n",
    "        # overwrite file first column to set the label name chosen in infiles (eg GAN, etc):\n",
    "        model_sequences[0] = model_name\n",
    "        sample_info[model_name] = pd.DataFrame({'label': model_sequences[0], 'ind': model_sequences[1]})\n",
    "        datasets[model_name] = np.array(model_sequences.loc[:, 2:].astype(int))\n",
    "\n",
    "        # transformations can be maf filtering, recoding into major=0/minor=1 format\n",
    "        if transformations is not None:\n",
    "            datasets[model_name], model_keep_all_snps[model_name] = datatransform(datasets[model_name],\n",
    "                                                                                  **transformations)\n",
    "        print(model_name, datasets[model_name].shape)\n",
    "    extra_sample_info = pd.DataFrame(np.concatenate(list(sample_info.values())), columns=['label', 'id'])\n",
    "    print(\"Dictionary of datasets:\", len(datasets))\n",
    "    return extra_sample_info, sample_info, datasets, transformations, model_keep_all_snps, number_of_samples\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "extra_sample_info, sample_info, datasets, transformations, model_keep_all_snps, number_of_samples = load_analysis_data_for_preview_tests(\n",
    "    models_to_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum_alleles_by_position, allele_frequency, is_fixed = build_allele_frequency(datasets)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LD Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"* Computing and plotting LD...\")\n",
    "#### Compute correlation between all pairs of SNPs for each generated/real dataset\n",
    "\n",
    "model_names = models_to_data.keys()\n",
    "hcor_snp = dict()\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(model_name)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Catch warnings due to fixed sites in dataset (the correlation value will be np.nan for pairs involving these sites)\n",
    "        hcor_snp[model_name] = np.corrcoef(datasets[model_name], rowvar=False) ** 2  # r2\n",
    "\n",
    "_, region_len, snps_on_same_chrom = get_dist(f\"../{REAL_POSITION_FILE_NAME}\", region_len_only=True,\n",
    "                                             kept_preprocessing=model_keep_all_snps['Real'])\n",
    "\n",
    "nbins = 100\n",
    "logscale = True\n",
    "bins = nbins\n",
    "binsPerDist = nbins\n",
    "if logscale: binsPerDist = np.logspace(np.log(1), np.log(region_len), nbins)\n",
    "\n",
    "# Compute LD binned by distance\n",
    "# Take only sites that are SNPs in all datasets (intersect)\n",
    "# (eg intersection of SNPs in Real, SNPs in GAN, SNPs in RBM etc)\n",
    "# -> Makes sense only if there is a correspondence between sites\n",
    "\n",
    "binnedLD = dict()\n",
    "binnedPerDistLD = dict()\n",
    "kept_snp = ~is_fixed\n",
    "n_kept_snp = np.sum(kept_snp)\n",
    "realdist = get_dist(f\"../{REAL_POSITION_FILE_NAME}\", kept_preprocessing=model_keep_all_snps['Real'],\n",
    "                    kept_snp=kept_snp)[0]\n",
    "mat = hcor_snp['Real']\n",
    "# filter and flatten\n",
    "flatreal = (mat[np.ix_(kept_snp, kept_snp)])[np.triu_indices(n_kept_snp)]\n",
    "isnanReal = np.isnan(flatreal)\n",
    "i = 1\n",
    "plt.figure(figsize=(10, len(hcor_snp) * 5))\n",
    "\n",
    "for model_name, mat in hcor_snp.items():\n",
    "    flathcor = (mat[np.ix_(kept_snp, kept_snp)])[np.triu_indices(n_kept_snp)]\n",
    "    isnan = np.isnan(flathcor)\n",
    "    curr_dist = realdist\n",
    "\n",
    "    # For each dataset LD pairs are stratified by SNP distance and cut into 'nbins' bins\n",
    "    # bin per SNP distance\n",
    "    ld = binned_statistic(curr_dist[~isnan], flathcor[~isnan], statistic='mean', bins=binsPerDist)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)  # so that empty bins do not raise a warning\n",
    "        binnedPerDistLD[model_name] = pd.DataFrame({'bin_edges': ld.bin_edges[:-1],\n",
    "                                                    'LD': ld.statistic,\n",
    "                                                    # 'sd': binned_statistic(curr_dist[~isnan], flathcor[~isnan], statistic = 'std', bins=binsPerDist).statistic,\n",
    "                                                    'sem': binned_statistic(curr_dist[~isnan], flathcor[~isnan],\n",
    "                                                                            statistic=sem,\n",
    "                                                                            bins=binsPerDist).statistic,\n",
    "                                                    'model_name': model_name, 'logscale': logscale})\n",
    "\n",
    "    # For each dataset LD pairs are stratified by LD values in Real and cut into 'nbins' bins\n",
    "    # binnedLD contains the average, std of LD values in each bin\n",
    "    isnan = np.isnan(flathcor) | np.isnan(flatreal)\n",
    "    ld = binned_statistic(flatreal[~isnan], flathcor[~isnan], statistic='mean', bins=bins)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)  # so that empty bins do not raise a warning\n",
    "        binnedLD[model_name] = pd.DataFrame({'bin_edges': ld.bin_edges[:-1],\n",
    "                                             'LD': ld.statistic,\n",
    "                                             'sd': binned_statistic(flatreal[~isnan], flathcor[~isnan],\n",
    "                                                                    statistic='std',\n",
    "                                                                    bins=bins).statistic,\n",
    "                                             'sem': binned_statistic(flatreal[~isnan], flathcor[~isnan],\n",
    "                                                                     statistic=sem,\n",
    "                                                                     bins=bins).statistic,\n",
    "                                             'model_name': model_name, 'logscale': logscale})\n",
    "\n",
    "    # Plotting quantiles ?\n",
    "    plotregquant(x=flatreal, y=flathcor,\n",
    "                 keys=['Real', model_name], statname='LD', col=color_palette[model_name],\n",
    "                 step=0.05,\n",
    "                 ax=plt.subplot(len(hcor_snp), 2, i))\n",
    "    i += 1\n",
    "    plt.title(f'Quantiles LD {model_name} vs Real')\n",
    "\n",
    "    # removing nan values and subsampling before doing the regression to have a reasonnable number of points\n",
    "    isnanInter = isnanReal | isnan\n",
    "    keepforplotreg = random.sample(list(np.where(~isnanInter)[0]), number_of_samples)\n",
    "    plotreg(x=flatreal[keepforplotreg], y=flathcor[keepforplotreg],\n",
    "            keys=['Real', model_name], statname='LD', col=color_palette[model_name],\n",
    "            ax=plt.subplot(len(hcor_snp), 2, i))\n",
    "    i += 1\n",
    "    plt.title(f'LD {model_name} vs Real')\n",
    "plt.savefig(os.path.join(output_dir, \"LD_generated_vs_real_intersectSNP.pdf\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import binned_statistic, sem\n",
    "import warnings\n",
    "\n",
    "\n",
    "def compute_and_plot_ld(real_data, synthetic_data, output_dir):\n",
    "    model_names = synthetic_data.keys()\n",
    "    hcor_snp = dict()\n",
    "\n",
    "    for model_name in model_names:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            hcor_snp[model_name] = np.corrcoef(real_data[model_name], rowvar=False) ** 2  # r2\n",
    "\n",
    "    _, region_len, snps_on_same_chrom = get_dist(f\"../{REAL_POSITION_FILE_NAME}\", region_len_only=True,\n",
    "                                                 kept_preprocessing=real_data)\n",
    "\n",
    "    nbins = 100\n",
    "    logscale = True\n",
    "    bins = nbins\n",
    "    binsPerDist = nbins\n",
    "    if logscale:\n",
    "        binsPerDist = np.logspace(np.log(1), np.log(region_len), nbins)\n",
    "\n",
    "    binnedLD = dict()\n",
    "    binnedPerDistLD = dict()\n",
    "    realdist = get_dist(f\"../{REAL_POSITION_FILE_NAME}\", kept_preprocessing=real_data,\n",
    "                        kept_snp='all')[0]\n",
    "    mat = hcor_snp['Real']\n",
    "    flatreal = (mat[np.ix_(kept_snp, kept_snp)])[np.triu_indices(n_kept_snp)]\n",
    "    isnanReal = np.isnan(flatreal)\n",
    "    i = 1\n",
    "\n",
    "    plt.figure(figsize=(10, len(hcor_snp) * 5))\n",
    "\n",
    "    for model_name, mat in hcor_snp.items():\n",
    "        flathcor = (mat[np.ix_(kept_snp, kept_snp)])[np.triu_indices(n_kept_snp)]\n",
    "        isnan = np.isnan(flathcor)\n",
    "        curr_dist = realdist\n",
    "\n",
    "        ld = binned_statistic(curr_dist[~isnan], flathcor[~isnan], statistic='mean', bins=binsPerDist)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            binnedPerDistLD[model_name] = pd.DataFrame({'bin_edges': ld.bin_edges[:-1],\n",
    "                                                        'LD': ld.statistic,\n",
    "                                                        'sem': binned_statistic(curr_dist[~isnan], flathcor[~isnan],\n",
    "                                                                                statistic=sem,\n",
    "                                                                                bins=binsPerDist).statistic,\n",
    "                                                        'model_name': model_name, 'logscale': logscale})\n",
    "\n",
    "        isnan = np.isnan(flathcor) | np.isnan(flatreal)\n",
    "        ld = binned_statistic(flatreal[~isnan], flathcor[~isnan], statistic='mean', bins=bins)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            binnedLD[model_name] = pd.DataFrame({'bin_edges': ld.bin_edges[:-1],\n",
    "                                                 'LD': ld.statistic,\n",
    "                                                 'sd': binned_statistic(flatreal[~isnan], flathcor[~isnan],\n",
    "                                                                        statistic='std',\n",
    "                                                                        bins=bins).statistic,\n",
    "                                                 'sem': binned_statistic(flatreal[~isnan], flathcor[~isnan],\n",
    "                                                                         statistic=sem,\n",
    "                                                                         bins=bins).statistic,\n",
    "                                                 'model_name': model_name, 'logscale': logscale})\n",
    "\n",
    "        # Plotting quantiles ?\n",
    "        plotregquant(x=flatreal, y=flathcor,\n",
    "                     keys=['Real', model_name], statname='LD', col=color_palette[model_name],\n",
    "                     step=0.05,\n",
    "                     ax=plt.subplot(len(hcor_snp), 2, i))\n",
    "        i += 1\n",
    "        plt.title(f'Quantiles LD {model_name} vs Real')\n",
    "\n",
    "        # removing nan values and subsampling before doing the regression to have a reasonnable number of points\n",
    "        isnanInter = isnanReal | isnan\n",
    "        keepforplotreg = random.sample(list(np.where(~isnanInter)[0]), number_of_samples)\n",
    "        plotreg(x=flatreal[keepforplotreg], y=flathcor[keepforplotreg],\n",
    "                keys=['Real', model_name], statname='LD', col=color_palette[model_name],\n",
    "                ax=plt.subplot(len(hcor_snp), 2, i))\n",
    "        i += 1\n",
    "        plt.title(f'LD {model_name} vs Real')\n",
    "    plt.savefig(os.path.join(output_dir, \"LD_generated_vs_real_intersectSNP.pdf\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AATS Privacy Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "line_styles = ['solid', 'dashdot', 'dotted']\n",
    "scores = []\n",
    "real_bld = binnedPerDistLD['Real'].LD.values[~np.isnan(binnedPerDistLD['Real'].LD.values)]\n",
    "# Calculate the absolute difference from the \"Real\" line\n",
    "for index, (model_name, bld) in enumerate(binnedPerDistLD.items()):\n",
    "    style_index = index % len(line_styles)\n",
    "    line_style = line_styles[style_index]\n",
    "    r2 = round(r2_score(real_bld, bld.LD.values[~np.isnan(bld.LD.values)]), 3)\n",
    "    rmse = round(np.sqrt(mean_squared_error(real_bld, bld.LD.values[~np.isnan(bld.LD.values)])), 3)\n",
    "    plt.errorbar(\n",
    "        bld.bin_edges.values, bld.LD.values, bld['sem'].values,\n",
    "        label=r\"$\\mathbf{\" + model_name + \"}$  RMSE = \" + str(rmse) + \", R-squared = \" + str(r2),\n",
    "        alpha=0.8, linewidth=3, linestyle=line_style\n",
    "    )\n",
    "\n",
    "# plt.title(\"Binned LD +/- 1 sem\")\n",
    "if logscale:\n",
    "    plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.xlabel(\"Distance between SNPs (bp) [Left bound of distance bin]\", fontsize=15)\n",
    "plt.ylabel(\"Average LD in bin\", fontsize=15)\n",
    "plt.legend(fontsize='x-large', loc=\"upper right\")\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"correlation_vs_dist_intersectSNP.jpg\"), bbox_inches='tight', dpi=500)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axes object\n",
    "fig, axes = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the data for each model\n",
    "for model_name, bld in binnedPerDistLD.items():\n",
    "    plt.errorbar(bld.bin_edges.values, bld.LD.values, bld['sem'].values, label=model_name, alpha=.65,\n",
    "                 linewidth=3, color=color_palette[model_name])\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title(\"Binned LD +/- 1 sem\")\n",
    "\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Distance between SNPs (bp) [Left bound of distance bin]\")\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Average LD in bin\")\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_dir, \"correlation_vs_dist_intersectSNP.pdf\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For each dataset LD pairs were stratified by LD values in Real, cut into nbins bins\n",
    "# binnedLD contains the average LD in each bin\n",
    "# Plot generated average LD as a function of the real average LD in the bins\n",
    "plt.figure(figsize=(10, 10))\n",
    "for model_name, bld in binnedLD.items():\n",
    "    plt.errorbar(bld.bin_edges.values, bld.LD.values, bld['sem'].values, label=model_name, alpha=0.8, marker='o')\n",
    "plt.title(\"Binned LD +/- 1 sem\")\n",
    "plt.xlabel(\"Bins (LD in Real)\")\n",
    "plt.ylabel(\"Average LD in bin\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'LD decay.jpg'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dSS_dic = dict()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "for cat, mat in datasets.items():\n",
    "    dAB = distance.cdist(mat, mat, 'cityblock')\n",
    "    np.fill_diagonal(dAB, np.Inf)\n",
    "    dSS_dic[cat] = dAB.min(axis=1)\n",
    "    sns.kdeplot(dAB[np.triu_indices(dAB.shape[0], k=1)], linewidth=3, label=cat)  # dSS\n",
    "plt.title(\"Pairwise distance within each dataset\")\n",
    "plt.legend(fontsize='x-large')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for cat, d in dSS_dic.items():\n",
    "    sns.kdeplot(dSS_dic[cat], linewidth=3, label=cat)\n",
    "plt.title(\"Minimal pairwise distance within each dataset\")\n",
    "plt.legend(fontsize='x-large')\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"haplo_pairw_distrib_within.pdf\"), bbox_inches='tight', dpi=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "haplo = np.concatenate(list(datasets.values())).T  # orientation of scikit allele\n",
    "\n",
    "outFilePrefix = ''\n",
    "# if not ref in model_name_to_input_file.keys(): continue\n",
    "ref = 'Real'\n",
    "print(\"Computing AATS with ref \" + ref)\n",
    "AA, MINDIST = computeAAandDist(\n",
    "    pd.DataFrame(haplo.T),\n",
    "    extra_sample_info.label,\n",
    "    models_to_data.keys(),\n",
    "    refCateg=ref,\n",
    "    saveAllDist=True,\n",
    "    output_dir=output_dir,\n",
    "    outFilePrefix=outFilePrefix)\n",
    "\n",
    "# save AA and MINDIST pd.DataFrame to csv\n",
    "# np.array of all pariwise distances are saved as npz automatically when calling computeAAandDist with saveAllDist=True\n",
    "AA.to_csv(os.path.join(output_dir, f'AA_{ref}.csv.bz2'), index=None)\n",
    "MINDIST.to_csv(os.path.join(output_dir, f'MINDIST_{ref}.csv.bz2'), index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Distribution WITHIN model_namesories\n",
    "W = pd.DataFrame(columns=['stat', 'statistic', 'label', 'comparaison'])\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "model_names = models_to_data.keys()\n",
    "for i, model_name in enumerate(model_names):\n",
    "    subset = (np.load('{}/dist_{}_{}.npz'.format(output_dir, model_name, model_name)))['dist']\n",
    "    if model_name == 'Real':\n",
    "        subsetreal = subset\n",
    "    sns.kdeplot(subset, linewidth=3, label=model_name)\n",
    "\n",
    "    sc = scs.wasserstein_distance(subsetreal, subset)\n",
    "    new_row = pd.DataFrame(\n",
    "        {'stat': ['wasserstein'], 'statistic': [sc], 'label': [model_name], 'comparaison': ['within']})\n",
    "    W = pd.concat([W, new_row], ignore_index=True)\n",
    "\n",
    "plt.legend(loc='upper left', fontsize='x-large')\n",
    "plt.savefig(os.path.join(output_dir, \"distribution_haplotypic_pairwise_diff.jpg\"), bbox_inches='tight', dpi=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Distribution WITHIN model_namesories\n",
    "W = pd.DataFrame(columns=['stat', 'statistic', 'label', 'comparaison'])\n",
    "\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = models_to_data.keys()\n",
    "for i, model_name in enumerate(model_names):\n",
    "    subset = (np.load('{}/dist_{}_{}.npz'.format(output_dir, model_name, model_name)))['dist']\n",
    "    if model_names == 'Real':\n",
    "        subsetreal = subset\n",
    "    sns.distplot(subset, hist=False, kde=True,\n",
    "                 kde_kws={'linewidth': 3},  #'bw':.02\n",
    "                 label='{} ({} identical pairs)'.format(model_names, (subset == 0).sum()))\n",
    "\n",
    "    sc = scs.wasserstein_distance(subsetreal, subset)\n",
    "    W = pd.concat([W, pd.DataFrame(\n",
    "        [{'stat': 'wasserstein', 'statistic': sc, 'pvalue': None, 'label': model_name, 'comparaison': 'between'}])],\n",
    "                  ignore_index=True)\n",
    "\n",
    "plt.title(\"Distribution of haplotypic pairwise difference within each dataset\")\n",
    "plt.legend()\n",
    "#plt.savefig(outDir+\"haplo_pairw_distrib_within_{}_simplify.pdf\".format(\"-\".join(categ)))\n",
    "subsetreal = None\n",
    "\n",
    "#### Distribution BETWEEN categories\n",
    "plt.subplot(1, 2, 2)\n",
    "model_names = models_to_data.keys()\n",
    "for i, model_name in enumerate(model_names):\n",
    "    subset = (np.load('{}/dist_{}_{}.npz'.format(output_dir, model_name, model_name)))['dist']\n",
    "    if model_name == 'Real':\n",
    "        subsetreal = subset\n",
    "    sns.distplot(subset, hist=False, kde=True,\n",
    "                 kde_kws={'linewidth': 3},  #'bw':.02\n",
    "                 label='{} vs {} ({} identical pairs)'.format(model_name, 'Real', (subset == 0).sum()))\n",
    "\n",
    "    sc = scs.wasserstein_distance(subsetreal, subset)\n",
    "    W = pd.concat([W, pd.DataFrame(\n",
    "        [{'stat': 'wasserstein', 'statistic': sc, 'pvalue': None, 'label': model_name, 'comparaison': 'between'}])])\n",
    "\n",
    "plt.title(\"Distribution of haplotypic pairwise difference between datasets\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, \"haplo_pairw_distrib.pdf\"))\n",
    "\n",
    "scores = pd.concat([W])\n",
    "\n",
    "print(W)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MINDIST.to_csv(os.path.join(output_dir, \"MINDIST.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_score_distributions(df):\n",
    "    # Create a figure with three subplots, one for each score type\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 15))\n",
    "\n",
    "    # Set the column names for the score types\n",
    "    score_types = ['dTS', 'dST', 'dSS']\n",
    "\n",
    "    # Set the colors for each model\n",
    "    model_colors = sns.color_palette('Set1', n_colors=len(df['cat'].unique()))\n",
    "\n",
    "    # Create a dictionary to store the model names and their corresponding colors\n",
    "    model_color_dict = dict(zip(df['cat'].unique(), model_colors))\n",
    "\n",
    "    # Iterate over the score types\n",
    "    for i, score_type in enumerate(score_types):\n",
    "        # Select the data for the current score type\n",
    "        data = df[['cat', score_type]]\n",
    "\n",
    "        # Melt the data to transform it into long format\n",
    "        data_melted = data.explode(score_type).reset_index(drop=True)\n",
    "\n",
    "        # Plot the distribution for each model\n",
    "        for model in df['cat'].unique():\n",
    "            model_data = data_melted[data_melted['cat'] == model]\n",
    "            color = model_color_dict[model]\n",
    "\n",
    "            sns.histplot(data=model_data, x=score_type, element='step', stat='density',\n",
    "                         common_norm=False, fill=False, kde=True,\n",
    "                         ax=axes[i], color=color, label=model)\n",
    "\n",
    "        # Set plot title and labels\n",
    "        axes[i].set_title(f'Distribution of {score_type}')\n",
    "        axes[i].set_xlabel('Score')\n",
    "        axes[i].set_ylabel('Density')\n",
    "\n",
    "        # Set legend\n",
    "        axes[i].legend(title='Model', loc='upper right')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_score_distributions(MINDIST)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_score_distributions(df, score_name):\n",
    "    flattened_df = df[['cat', score_name]].explode(score_name).reset_index(drop=True)\n",
    "\n",
    "    # Reorder the unique model names with 'Real' at the front\n",
    "    unique_models = list(flattened_df[\"cat\"].unique())\n",
    "    unique_models.remove('Real')\n",
    "    unique_models.append('Real')\n",
    "    color_palette['Real'] = 'black'\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Iterate over the reordered unique model names and plot the distribution for each\n",
    "    for model_name in unique_models:\n",
    "        if model_name == 'Real':\n",
    "            sns.kdeplot(data=flattened_df[flattened_df[\"cat\"] == model_name], x=score_name, label=model_name,\n",
    "                        fill=True, common_norm=False, alpha=0.6, color='black')\n",
    "        else:\n",
    "            sns.kdeplot(data=flattened_df[flattened_df[\"cat\"] == model_name], x=score_name, label=model_name,\n",
    "                        common_norm=False, alpha=1, linewidth=5)\n",
    "\n",
    "    plt.xlabel(score_name)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(title=\"Model Name\", loc=\"upper right\")\n",
    "    plt.title(f\"Distribution of {score_name} by Model\")\n",
    "    plt.savefig(os.path.join(output_dir, score_name + \"_DISTRIBUTIONS.jpg\"))\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_score_distributions(MINDIST, 'dST')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_score_distributions(MINDIST, 'dTS')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_score_distributions(MINDIST, 'dSS')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W = pd.DataFrame(columns=['stat', 'statistic', 'label', 'comparaison'])\n",
    "for model_name in models_to_data.keys():\n",
    "    for method in ['dTS', 'dST', 'dSS']:\n",
    "        real = MINDIST[method][MINDIST.cat == 'Real'][0]\n",
    "        sc = scs.wasserstein_distance(real, MINDIST[method][MINDIST.cat == model_name].values[0])\n",
    "        new_row = pd.DataFrame({'stat': ['wasserstein'], 'statistic': [sc],\n",
    "                                'label': [model_name], 'comparaison': [method]})\n",
    "        W = pd.concat([W, new_row], ignore_index=True)\n",
    "scores = pd.concat([W])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = pd.concat([scores, W])\n",
    "scores.to_csv(os.path.join(output_dir, \"scores_pairwise_distances.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(1.5 * len(model_names), 6))\n",
    "\n",
    "sns.barplot(x='Cat', y='Value', hue='Variable', palette=sns.color_palette('colorblind'),\n",
    "            data=(AA.drop(columns=['PrivacyLoss', 'ref'], errors='ignore')).melt(id_vars='cat').rename(\n",
    "                columns=str.title))\n",
    "plt.axhline(0.5, color='black')\n",
    "if 'Real_test' in AA.cat.values:\n",
    "    plt.axhline(np.float(AA[AA.cat == 'Real_test'].AATS), color=sns.color_palette()[0], ls='--')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.title(\"Nearest Neighbor Adversarial Accuracy on training (AATS) and its components\")\n",
    "plt.savefig(os.path.join(output_dir, \"AATS_scores.pdf\"))\n",
    "\n",
    "Test = '_Test2'\n",
    "Train = ''  # means Training set is Real\n",
    "dfPL = plotPrivacyLoss(Train, Test, output_dir, color_palette, color_palette)\n",
    "\n",
    "Test = '_Test2'\n",
    "Train = '_Test1'\n",
    "dfPL = plotPrivacyLoss(Train, Test, output_dir, color_palette, color_palette)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def plot_3corr(x, y, keys, statname, col, ax=None):\n",
    "    \"\"\"\n",
    "    Plot for x versus y with regression scores and returns correlation coefficient\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array, scalar\n",
    "    y : array, scalar\n",
    "    statname : str\n",
    "        'Allele frequency' LD' or '3 point correlation' etc.\n",
    "    col : str, color code\n",
    "        color\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lims = [np.min(x), np.max(x)]\n",
    "    r, _ = pearsonr(x, y)\n",
    "    if sm_loaded:\n",
    "        reg = sm.OLS(x, y).fit()\n",
    "    if ax is None:\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "    if len(x) < 100:\n",
    "        alpha = 1\n",
    "    else:\n",
    "        alpha = .6\n",
    "    ax.plot(x, y, label=f\"{keys[1]}: cor={round(r, 2)}\", c=col, marker='o', lw=0, alpha=alpha)\n",
    "    ax.plot(lims, lims, ls='--', alpha=1, c='black')\n",
    "    ax.set_xlabel(f'{statname} in {keys[0]}')\n",
    "    ax.set_ylabel(f'{statname} in {keys[1]}')\n",
    "\n",
    "    return r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Points Correlation Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reduced_dataset = {'Real': datasets['Real'], 'GAN 2019 Retrain': datasets['GAN 2019 Retrain'],\n",
    "                   'Genome-AC-GAN By Continental Population': datasets['Genome-AC-GAN By Continental Population']}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_counts(haplosubset, points):\n",
    "    counts = np.unique(\n",
    "        np.apply_along_axis(\n",
    "            lambda x: ''.join(map(str, x[points])),\n",
    "            # lambda x: ''.join([str(x[p]) for p in points]),\n",
    "            0, haplosubset),\n",
    "        return_counts=True)\n",
    "    return (counts)\n",
    "\n",
    "\n",
    "def get_frequencies(counts):\n",
    "    l = len(counts[0][0])  # haplotype length\n",
    "    nind = np.sum(counts[1])\n",
    "    f = np.zeros(shape=[2] * l)\n",
    "    for i, allele in enumerate(counts[0]):\n",
    "        f[tuple(map(int, allele))] = counts[1][i] / nind\n",
    "    return f\n",
    "\n",
    "\n",
    "def three_points_cor(haplosubset, out='all'):\n",
    "    F = dict()\n",
    "    for points in [[0], [1], [2], [0, 1], [0, 2], [1, 2], [0, 1, 2]]:\n",
    "        strpoints = ''.join(map(str, points))\n",
    "        F[strpoints] = get_frequencies(\n",
    "            get_counts(haplosubset, points)\n",
    "        )\n",
    "\n",
    "    cors = [\n",
    "        F['012'][a, b, c] - F['01'][a, b] * F['2'][c] - F['12'][b, c] * F['0'][a] - F['02'][a, c] * F['1'][b] + 2 *\n",
    "        F['0'][a] * F['1'][b] * F['2'][c] for a, b, c in itertools.product(*[[0, 1]] * 3)]\n",
    "    if out == 'mean':\n",
    "        return (np.mean(cors))\n",
    "    if out == 'max':\n",
    "        return (np.max(np.abs(cors)))\n",
    "    if out == 'all':\n",
    "        return (cors)\n",
    "    return (ValueError(f\"out={out} not recognized\"))\n",
    "\n",
    "\n",
    "# def mult_three_point_cor(haplo, extra_sample_info, model_name, picked_three_points):\n",
    "#    return [three_points_cor(haplo[np.ix_(snps,extra_sample_info.label==model_name)], out='all') for snps in picked_three_points]\n",
    "\n",
    "# set the seed so that the same real individual are subsampled (when needed)\n",
    "# to ensure consistency of the scores when adding a new model or a new sumstat\n",
    "np.random.seed(3)\n",
    "random.seed(3)\n",
    "\n",
    "# Compute 3 point correlations results for different datasets and different distances between SNPs\n",
    "\n",
    "# pick distance between SNPs at which 3point corr will be computed\n",
    "# (defined in nb of snps)\n",
    "# a gap of -9 means that snp triplets are chosen completely at random (not predefined distance)\n",
    "# for each category we randomly pick 'nsamplesets' triplets\n",
    "\n",
    "# if datasets have different nb of snps, for convenience we will sample\n",
    "# slightly more at the beginning of the chunk\n",
    "\n",
    "gap_vec = [1, 4, 16, 64, 256, 512, 1024, -9]\n",
    "nsamplesets = 1000\n",
    "min_nsnp = min([dat.shape[1] for dat in reduced_dataset.values()])\n",
    "cors_meta = dict()\n",
    "for gap in gap_vec:\n",
    "    print(f'\\n gap={gap} SNPs', end=' ')\n",
    "    if gap < 0:\n",
    "        # pick 3 random snps\n",
    "        picked_three_points = [random.sample(range(min_nsnp), 3) for _ in range(nsamplesets)]\n",
    "    else:\n",
    "        try:\n",
    "            # pick 3 successive snps spearated by 'gap' SNPs\n",
    "            step = gap + 1\n",
    "            picked_three_points = [np.asarray(random.sample(range(min_nsnp - 2 * step), 1)) + [0, step, 2 * step]\n",
    "                                   for _\n",
    "                                   in range(nsamplesets)]\n",
    "        except:\n",
    "            continue  # if there were not enough SNPs for this gap\n",
    "    cors = dict()\n",
    "\n",
    "    for model_name in reduced_dataset.keys():\n",
    "        print(model_name, end=' ')\n",
    "        # cors[model_name]=[three_points_cor(haplo[np.ix_(snps,extra_sample_info.label==model_name)], out='all') for snps in picked_three_points]\n",
    "        cors[model_name] = [three_points_cor(reduced_dataset[model_name][:, snps].T, out='all') for snps in\n",
    "                            picked_three_points]\n",
    "\n",
    "    cors_meta[gap] = cors.copy()\n",
    "\n",
    "# print(cors_meta)\n",
    "\n",
    "with open(os.path.join(output_dir, \"3pointcorr.pkl\"), \"wb\") as outfile:\n",
    "    pickle.dump(cors_meta, outfile)\n",
    "\n",
    "plt.figure(figsize=(7 * len(cors_meta), 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for i, gap in enumerate((cors_meta).keys()):\n",
    "    ax = plt.subplot(2, int(np.ceil(len(cors_meta) / 2)), int(i) + 1)\n",
    "    cors = cors_meta[gap]\n",
    "    real = list(np.array(cors['Real']).flat)\n",
    "    lims = [np.min(real), np.max(real)]\n",
    "    model_to_corr = {}\n",
    "    for key, val in cors.items():\n",
    "        if key == 'Real': continue\n",
    "        val = list(np.array(val).flat)\n",
    "        corr = plot_3corr(x=real, y=val, keys=['Real', key],\n",
    "                       statname='Correlation', col=color_palette[key], ax=ax)\n",
    "        ax.set_ylabel(f'Correlation In Synthetic', fontsize=30)\n",
    "        ax.set_xlabel(f'Correlation In Real', fontsize=30)\n",
    "        ax.set_xlim((-.1, .1))\n",
    "        ax.set_ylim((-.1, .1))\n",
    "        model_to_corr[key] = corr\n",
    "\n",
    "    corr_size = str(gap) if gap > 0 else \"Random\"\n",
    "    title = [f\"3point Correlation By {corr_size} SNPs\"]\n",
    "    for model_name, corr_values in model_to_corr.items():\n",
    "        model_name_display = model_name.replace(\"Population\", \"\").replace(\"By \", \"\")\n",
    "        title.append(f\"{model_name}:{corr_values*100: .1f}%\")\n",
    "    title = \"\\n\".join(title)\n",
    "    plt.title(title, fontsize=29, y=1.05, fontweight='bold')\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, '3point_correlations_fixlim.jpg'), bbox_inches='tight', dpi=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
