{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from utils.util import *\n",
    "%matplotlib inline\n",
    "\n",
    "discriminator_folder = \"../experiment_results/discriminator_0.8_test\"\n",
    "csv_name = \"discriminator_pred_on_test.csv\"\n",
    "output_dir = \"classifier_analysis\"\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"categorical_crossentropy\": [\"../experiment_results/categorical_crossentropy/discriminator_metrics.csv\",\n",
    "                                 \"../experiment_results/categorical_crossentropy_02/discriminator_metrics.csv\",\n",
    "                                 \"../experiment_results/categorical_crossentropy_03/discriminator_metrics.csv\"],\n",
    "    \"Polyloss_CE\": [\"../experiment_results/polyloss_ce/discriminator_metrics.csv\",\n",
    "                    \"../experiment_results/polyloss_ce_02/discriminator_metrics.csv\",\n",
    "                    \"../experiment_results/polyloss_ce_03/discriminator_metrics.csv\"]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def experiment_name_to_avg_df_without_outliers(experiments):\n",
    "    results = {}\n",
    "    for experiments_name, paths in experiments.items():\n",
    "        df_list = []\n",
    "        for path in paths:\n",
    "            df_list.append(pd.read_csv(path))\n",
    "        df = pd.concat(df_list, ignore_index=True)[[\"Accuracy\", \"F1-score\", \"epoch\"]]\n",
    "        before_len = len(df)\n",
    "        # Sort the DataFrame by the \"Accuracy\" column in descending order\n",
    "        df_sorted = df.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "        # Calculate the number of rows to remove (1% of the total rows)\n",
    "        n_rows = int(len(df_sorted) * 0.025)\n",
    "\n",
    "        # Remove the top and bottom rows\n",
    "        df = df_sorted.iloc[n_rows:-n_rows]\n",
    "\n",
    "        # Reset the index of the filtered DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        after_len = len(df)\n",
    "        print(f\"{experiments_name}: removed {before_len - after_len} samples from {before_len}\")\n",
    "        # df = df.groupby('epoch').agg('mean')\n",
    "        # epochs = df.index\n",
    "        # df = df.reset_index()\n",
    "        # df[\"epoch\"] = epochs\n",
    "        results[experiments_name] = df\n",
    "    return results\n",
    "\n",
    "\n",
    "experiment_name_to_avg_df = experiment_name_to_avg_df_without_outliers(experiments)\n",
    "experiment_name_to_avg_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_loss_comparison(loss_function_to_df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each loss function and its corresponding dataframe\n",
    "    for loss_function, df in loss_function_to_df.items():\n",
    "        df = df.sort_values(\"epoch\")\n",
    "\n",
    "        epochs = df[\"epoch\"]\n",
    "        accuracy = df[\"Accuracy\"].rolling(1).mean()\n",
    "\n",
    "        # Calculate standard deviation of accuracy values\n",
    "        accuracy_std = np.std(accuracy)\n",
    "        accuracy_mean = np.mean(accuracy)\n",
    "        # Plot the Accuracy by epoch for each loss function\n",
    "        ax.plot(epochs, accuracy, label=f\"{loss_function} (Mean: {accuracy_mean:.4f}, Std: {accuracy_std:.4f})\")\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "    ax.set_title(\"Accuracy Comparison by Loss Function\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_loss_comparison(loss_function_to_df):\n",
    "    # Create a list to store the accuracy values for each loss function\n",
    "    accuracy_data = []\n",
    "\n",
    "    # Iterate over each loss function and its corresponding dataframe\n",
    "    for loss_function, df in loss_function_to_df.items():\n",
    "        df = df.sort_values(\"epoch\")\n",
    "        accuracy = df[\"Accuracy\"]\n",
    "        accuracy_data.append(accuracy)\n",
    "\n",
    "    # Plot the boxplot for the accuracy data\n",
    "    fig, ax = plt.subplots(figsize=(5, 6))\n",
    "\n",
    "    # Define the boxplot properties\n",
    "    boxprops = dict(linewidth=2, color='black')\n",
    "    medianprops = dict(linewidth=2, color='black')\n",
    "    meanprops = dict(linewidth=2, color='black')\n",
    "\n",
    "    # Create the boxplot\n",
    "    boxplot = ax.boxplot(accuracy_data, labels=loss_function_to_df.keys(), patch_artist=True,\n",
    "                         showfliers=False, boxprops=boxprops, medianprops=medianprops, meanprops=meanprops)\n",
    "\n",
    "    # Set a different color for each boxplot\n",
    "    colors = ['Blue', 'Orange']\n",
    "    for patch, color in zip(boxplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # Add labels and title to the plot\n",
    "    # ax.set_xlabel(\"Loss Function\", fontsize=12)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"box_plot_loss_function_classes.jpg\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss_comparison(experiment_name_to_avg_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_accuracy_list(experiment_name_to_avg_df):\n",
    "    accuracy = []\n",
    "    # Plot accuracy and precision for each algorithm\n",
    "    for index, (experiments_name, df) in enumerate(experiment_name_to_avg_df.items()):\n",
    "        df = df.sort_values(\"epoch\")\n",
    "        accuracy.append(df[\"Accuracy\"])\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_lists = get_accuracy_list(experiment_name_to_avg_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming you have a tuple called 'accuracy' with two lists\n",
    "experiments_names = list(experiments.keys())\n",
    "\n",
    "# Plotting histograms for the two distributions\n",
    "plt.hist(accuracy_lists[0], bins=20, alpha=0.5, label=experiments_names[0], density=True)\n",
    "plt.hist(accuracy_lists[1], bins=20, alpha=0.5, label=experiments_names[1], density=True)\n",
    "\n",
    "# Adding labels and title to the plot\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(accuracy_lists[0], accuracy_lists[1])\n",
    "\n",
    "plt.title(f'p-value: {p_value:.8f}', fontweight='bold')\n",
    "\n",
    "# Move the legend to the upper right corner\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig(os.path.join(output_dir, \"accuracy_distribution.jpg\"), bbox_inches='tight', dpi=300)\n",
    "# Display the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def plot_classifications_confusion_matrix(file_paths, output_dir, experiments_rename, prefix=\"\"):\n",
    "    # Create an empty list to store the confusion matrices and evaluation scores\n",
    "    cm_list = []\n",
    "    kappa_list = []\n",
    "    accuracy_list = []\n",
    "    f1_score_list = []\n",
    "\n",
    "    # Loop through each file path\n",
    "    for i, path in enumerate(file_paths):\n",
    "        # Read in the CSV file\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        # Extract the actual and predicted population values from the dataframe\n",
    "        actual_pop = data['class_name_real'].values\n",
    "        predicted_pop = data['class_name_pred'].values\n",
    "\n",
    "        # Get the unique classes\n",
    "        classes = np.unique(actual_pop)\n",
    "\n",
    "        # Create the confusion matrix\n",
    "        cm = confusion_matrix(actual_pop, predicted_pop, labels=classes)\n",
    "\n",
    "        # Calculate evaluation scores\n",
    "        kappa = cohen_kappa_score(actual_pop, predicted_pop)\n",
    "        accuracy = accuracy_score(actual_pop, predicted_pop)\n",
    "        f1 = f1_score(actual_pop, predicted_pop, average='weighted')\n",
    "\n",
    "        # Append the confusion matrix and evaluation scores to the list\n",
    "        cm_list.append(cm)\n",
    "        kappa_list.append(kappa)\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_score_list.append(f1)\n",
    "\n",
    "    # Calculate the number of rows and columns for the subplots\n",
    "    n_plots = len(file_paths)\n",
    "    n_cols = min(2, n_plots)\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5 * n_rows))\n",
    "    # fig.suptitle('Confusion Matrix Comparison')\n",
    "\n",
    "    # Flatten the axes array for indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each confusion matrix and subplot\n",
    "    for i, (cm, kappa, accuracy, f1) in enumerate(zip(cm_list, kappa_list, accuracy_list, f1_score_list)):\n",
    "        ax = axes[i]\n",
    "        cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100  # Convert numbers to percentages\n",
    "        im = ax.imshow(cm_percentage, cmap=plt.cm.gray_r)  # Reverse the color scale\n",
    "        ax.set_title(f'{experiments_rename[i]}\\n\\nKappa: {kappa:.2f}, Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}',\n",
    "                     fontweight='bold')\n",
    "        ax.set_xlabel('Predicted Label')\n",
    "        ax.set_ylabel('True Label')\n",
    "        ax.set_xticks(np.arange(len(classes)))\n",
    "        ax.set_yticks(np.arange(len(classes)))\n",
    "        ax.set_xticklabels(classes)\n",
    "        ax.set_yticklabels(classes)\n",
    "        ax.grid(False)\n",
    "        # Add a colorbar\n",
    "        fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "        # Add the confusion matrix values as text annotations\n",
    "        for j in range(len(classes)):\n",
    "            for k in range(len(classes)):\n",
    "                c = \"black\" if cm_percentage[j, k] < 40 else \"white\"\n",
    "                w = \"normal\" if cm_percentage[j, k] < 40 else \"bold\"\n",
    "                text = ax.text(k, j, f'{cm[j, k]}\\n{cm_percentage[j, k]:.1f}%',\n",
    "                               ha='center', va='center', color=c, weight=w)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(output_dir, prefix + \"compare_confusion_matrix.jpg\"), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_classifications_confusion_matrix(\n",
    "    ['../experiment_results/categorical_crossentropy_02/discriminator_pred_on_test.csv',\n",
    "     '../experiment_results/polyloss_ce/discriminator_pred_on_test.csv'],\n",
    "    output_dir, list(experiments.keys()), prefix=\"polyloss_ce_to_cce_\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_last_metrics(experiment_name_to_avg_df):\n",
    "    last_metrics_dfs = []\n",
    "    for experiments_name, df in experiment_name_to_avg_df.items():\n",
    "        df = df[[\"Accuracy\", \"F1-score\"]]\n",
    "\n",
    "        def get_agg_df(total_df, tail):\n",
    "            tmp_df = total_df.tail(tail)\n",
    "            tmp_df.columns = [f\"Accuracy\\n last {tail}\", f\"F1-score\\n last {tail}\"]\n",
    "            return pd.DataFrame(tmp_df.mean()).T\n",
    "\n",
    "        df_1 = get_agg_df(df, 1)\n",
    "        df_5 = get_agg_df(df, 5)\n",
    "        df_10 = get_agg_df(df, 10)\n",
    "        df_20 = get_agg_df(df, 20)\n",
    "        df_50 = get_agg_df(df, 50)\n",
    "        df_100 = get_agg_df(df, 100)\n",
    "        output_df = pd.concat([df_1, df_5, df_10, df_20, df_50, df_100], axis=1)\n",
    "        output_df[\"experiments_name\"] = experiments_name\n",
    "        last_metrics_dfs.append(output_df)\n",
    "    # catboost_results = pd.read_csv(os.path.join(output_dir, \"catboost_classification_results.csv\"))\n",
    "    # catboost_results[\"experiments_name\"] = \"CatBoostClassifier\"\n",
    "    # last_metrics_dfs.append(catboost_results.tail(1))\n",
    "    return pd.concat(last_metrics_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "all_experiments_scores = get_last_metrics(experiment_name_to_avg_df)\n",
    "all_experiments_scores.set_index('experiments_name', inplace=True)\n",
    "\n",
    "all_experiments_scores = all_experiments_scores.round(4)\n",
    "\n",
    "df_transposed = all_experiments_scores.transpose()\n",
    "\n",
    "# plot the bar chart\n",
    "ax = df_transposed.plot(kind='bar', figsize=(30, 10), width=0.9, fontsize=16)\n",
    "\n",
    "# set labels\n",
    "ax.set_title('Performance of experiments - average recent epochs', fontsize=22)\n",
    "ax.set_xlabel('Score Type')\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "# show the legend\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "# add numbers to bars\n",
    "for i, rect in enumerate(ax.containers):\n",
    "    ax.bar_label(rect, labels=all_experiments_scores.iloc[i].astype(str), fontsize=16)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"discriminator_compare_last_x.jpg\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
