{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.util import *\n",
    "%matplotlib inline\n",
    "\n",
    "discriminator_folder = \"../experiment_results/discriminator_0.8_test\"\n",
    "csv_name = \"discriminator_pred_on_test.csv\"\n",
    "output_dir = \"classifier_analysis\"\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"categorical_crossentropy\": [\"../experiment_results/categorical_crossentropy/discriminator_metrics.csv\",\n",
    "                                 \"../experiment_results/categorical_crossentropy_02/discriminator_metrics.csv\",\n",
    "                                 \"../experiment_results/categorical_crossentropy_03/discriminator_metrics.csv\"],\n",
    "    \"Polyloss_CE\": [\"../experiment_results/polyloss_ce/discriminator_metrics.csv\",\n",
    "                    \"../experiment_results/polyloss_ce_02/discriminator_metrics.csv\",\n",
    "                    \"../experiment_results/polyloss_ce_03/discriminator_metrics.csv\"]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_crossentropy: removed 2 samples from 303\n",
      "Polyloss_CE: removed 7 samples from 303\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'categorical_crossentropy':      epoch  Accuracy  F1-score\n 0       50  0.862333  0.825562\n 1      100  0.841667  0.801931\n 2      150  0.833667  0.800052\n 3      200  0.819667  0.767970\n 4      250  0.815333  0.781866\n ..     ...       ...       ...\n 96    4850  0.839333  0.817602\n 97    4900  0.678667  0.628282\n 98    4950  0.844667  0.819537\n 99    5000  0.849667  0.825646\n 100   5001  0.835667  0.809387\n \n [101 rows x 3 columns],\n 'Polyloss_CE':      epoch  Accuracy  F1-score\n 0       50  0.857667  0.820465\n 1      100  0.860667  0.822256\n 2      150  0.821000  0.784572\n 3      200  0.858333  0.827406\n 4      250  0.761000  0.726644\n ..     ...       ...       ...\n 96    4850  0.867000  0.838243\n 97    4900  0.870333  0.846100\n 98    4950  0.864000  0.840443\n 99    5000  0.871000  0.842650\n 100   5001  0.863333  0.840205\n \n [101 rows x 3 columns]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def experiment_name_to_avg_df_without_outliers(experiments):\n",
    "    results = {}\n",
    "    for experiments_name, paths in experiments.items():\n",
    "        df_list = []\n",
    "        for path in paths:\n",
    "            df_list.append(pd.read_csv(path))\n",
    "        df = pd.concat(df_list, ignore_index=True)[[\"Accuracy\", \"F1-score\", \"epoch\"]]\n",
    "        Q5 = df[\"Accuracy\"].quantile(0.05)\n",
    "        Q95 = df[\"Accuracy\"].quantile(0.95)\n",
    "        IQR = Q95 - Q5\n",
    "        before_len = len(df)\n",
    "        df = df[~((df[[\"Accuracy\"]] < (Q5 - IQR)) | (df[[\"Accuracy\"]] > (Q95 + IQR))).any(axis=1)]\n",
    "        after_len = len(df)\n",
    "        print(f\"{experiments_name}: removed {before_len-after_len} samples from {before_len}\")\n",
    "        df = df.groupby('epoch').agg('mean')\n",
    "        epochs = df.index\n",
    "        df = df.reset_index()\n",
    "        df[\"epoch\"] = epochs\n",
    "        results[experiments_name] = df\n",
    "    return results\n",
    "\n",
    "experiment_name_to_avg_df = experiment_name_to_avg_df_without_outliers(experiments)\n",
    "experiment_name_to_avg_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_metrics(experiment_name_to_avg_df):\n",
    "    # Set up figure and axes\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(40, 20))\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    # Plot accuracy and precision for each algorithm\n",
    "    for experiments_name, df in experiment_name_to_avg_df.items():\n",
    "        accuracy.append(df[\"Accuracy\"])\n",
    "        f1.append(df[\"F1-score\"])\n",
    "        x = df[\"epoch\"]\n",
    "\n",
    "        # Compute cumulative average every 3 epochs\n",
    "        df[\"Accuracy_cumavg\"] = df[\"Accuracy\"].rolling(1).mean()\n",
    "        df[\"F1_score_cumavg\"] = df[\"F1-score\"].rolling(1).mean()\n",
    "\n",
    "        # Get the color of the line used for the experiment\n",
    "        color = ax[0]._get_lines.get_next_color()\n",
    "\n",
    "        # Add regression line for accuracy\n",
    "        ax[0].plot(x, df[\"Accuracy_cumavg\"], label=f\"{experiments_name}\", color=color, linestyle='dashdot')\n",
    "        coefficients = np.polyfit(x, df[\"Accuracy\"], 4)\n",
    "        p = np.poly1d(coefficients)\n",
    "        ax[0].plot(x, p(x), color=color, linewidth=4)\n",
    "\n",
    "        # Add regression line for precision\n",
    "        ax[1].plot(x, df[\"F1_score_cumavg\"], label=f\"{experiments_name}\", color=color, linestyle='dashdot')\n",
    "        coefficients = np.polyfit(x, df[\"F1-score\"], 4)\n",
    "        p = np.poly1d(coefficients)\n",
    "        ax[1].plot(x, p(x), color=color, linewidth=4)\n",
    "\n",
    "        # Add points to the accuracy graph\n",
    "        ax[0].scatter(x, df[\"Accuracy\"], color=color)\n",
    "\n",
    "        # Add points to the precision graph\n",
    "        ax[1].scatter(x, df[\"F1-score\"], color=color)\n",
    "\n",
    "        ax[0].legend(fontsize=30)\n",
    "        ax[0].set_xlabel(\"Epoch\", fontsize=30)\n",
    "        ax[0].set_ylabel(\"Accuracy\", fontsize=30)\n",
    "\n",
    "        ax[1].legend(fontsize=30)\n",
    "        ax[1].set_xlabel(\"Epoch\", fontsize=30)\n",
    "        ax[1].set_ylabel(\"F1-score\", fontsize=30)\n",
    "\n",
    "\n",
    "    ax[0].set_title(f\"Accuracy P Value = {round(1 - stats.ttest_ind(accuracy[0], accuracy[1])[1], 5)}\", fontsize=40)\n",
    "    ax[1].set_title(f\"F1-score P Value = {round(1 - stats.ttest_ind(f1[0], f1[1])[1], 5)}\", fontsize=40)\n",
    "    plt.savefig(os.path.join(output_dir, \"discriminator_class_metrics.jpg\"))\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metrics(experiment_name_to_avg_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def plot_classifications_confusion_matrix(file_paths, output_dir, experiments_rename, prefix=\"\"):\n",
    "    # Create an empty list to store the confusion matrices and evaluation scores\n",
    "    cm_list = []\n",
    "    kappa_list = []\n",
    "    accuracy_list = []\n",
    "    f1_score_list = []\n",
    "\n",
    "    # Loop through each file path\n",
    "    for i, path in enumerate(file_paths):\n",
    "        # Read in the CSV file\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        # Extract the actual and predicted population values from the dataframe\n",
    "        actual_pop = data['class_name_real'].values\n",
    "        predicted_pop = data['class_name_pred'].values\n",
    "\n",
    "        # Get the unique classes\n",
    "        classes = np.unique(actual_pop)\n",
    "\n",
    "        # Create the confusion matrix\n",
    "        cm = confusion_matrix(actual_pop, predicted_pop, labels=classes)\n",
    "\n",
    "        # Calculate evaluation scores\n",
    "        kappa = cohen_kappa_score(actual_pop, predicted_pop)\n",
    "        accuracy = accuracy_score(actual_pop, predicted_pop)\n",
    "        f1 = f1_score(actual_pop, predicted_pop, average='weighted')\n",
    "\n",
    "        # Append the confusion matrix and evaluation scores to the list\n",
    "        cm_list.append(cm)\n",
    "        kappa_list.append(kappa)\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_score_list.append(f1)\n",
    "\n",
    "    # Calculate the number of rows and columns for the subplots\n",
    "    n_plots = len(file_paths)\n",
    "    n_cols = min(2, n_plots)\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5 * n_rows))\n",
    "    # fig.suptitle('Confusion Matrix Comparison')\n",
    "\n",
    "    # Flatten the axes array for indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each confusion matrix and subplot\n",
    "    for i, (cm, kappa, accuracy, f1) in enumerate(zip(cm_list, kappa_list, accuracy_list, f1_score_list)):\n",
    "        ax = axes[i]\n",
    "        cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100  # Convert numbers to percentages\n",
    "        im = ax.imshow(cm_percentage, cmap=plt.cm.terrain)\n",
    "        ax.set_title(f'{experiments_rename[i]}\\n\\nKappa: {kappa:.2f}, Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}')\n",
    "        ax.set_xlabel('Predicted Label')\n",
    "        ax.set_ylabel('True Label')\n",
    "        ax.set_xticks(np.arange(len(classes)))\n",
    "        ax.set_yticks(np.arange(len(classes)))\n",
    "        ax.set_xticklabels(classes)\n",
    "        ax.set_yticklabels(classes)\n",
    "        ax.grid(False)\n",
    "        # Add a colorbar\n",
    "        fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "        # Add the confusion matrix values as text annotations\n",
    "        for j in range(len(classes)):\n",
    "            for k in range(len(classes)):\n",
    "                text = ax.text(k, j, f'{cm[j, k]}\\n{cm_percentage[j, k]:.1f}%',\n",
    "                               ha='center', va='center', color='black', weight='bold')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    # for i in range(n_plots, n_rows * n_cols):\n",
    "    #     axes[i].axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(os.path.join(output_dir, prefix + \"compare_confusion_matrix.jpg\"))\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_classifications_confusion_matrix(['../experiment_results/categorical_crossentropy_02/discriminator_pred_on_test.csv',\n",
    "                                       '../experiment_results/polyloss_ce/discriminator_pred_on_test.csv'],\n",
    "                                      output_dir, list(experiments.keys()), prefix=\"polyloss_ce_to_cce_\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_last_metrics(experiment_name_to_avg_df):\n",
    "    last_metrics_dfs = []\n",
    "    for experiments_name, df in experiment_name_to_avg_df.items():\n",
    "        df = df[[\"Accuracy\", \"F1-score\"]]\n",
    "        def get_agg_df(total_df, tail):\n",
    "            tmp_df = total_df.tail(tail)\n",
    "            tmp_df.columns = [f\"Accuracy\\n last {tail}\", f\"F1-score\\n last {tail}\"]\n",
    "            return pd.DataFrame(tmp_df.mean()).T\n",
    "        df_1 = get_agg_df(df, 1)\n",
    "        df_5 = get_agg_df(df, 5)\n",
    "        df_10 = get_agg_df(df, 10)\n",
    "        df_20 = get_agg_df(df, 20)\n",
    "        df_50 = get_agg_df(df, 50)\n",
    "        df_100 = get_agg_df(df, 100)\n",
    "        output_df = pd.concat([df_1, df_5, df_10, df_20, df_50, df_100], axis=1)\n",
    "        output_df[\"experiments_name\"] = experiments_name\n",
    "        last_metrics_dfs.append(output_df)\n",
    "    # catboost_results = pd.read_csv(os.path.join(output_dir, \"catboost_classification_results.csv\"))\n",
    "    # catboost_results[\"experiments_name\"] = \"CatBoostClassifier\"\n",
    "    # last_metrics_dfs.append(catboost_results.tail(1))\n",
    "    return pd.concat(last_metrics_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "all_experiments_scores = get_last_metrics(experiment_name_to_avg_df)\n",
    "all_experiments_scores.set_index('experiments_name', inplace=True)\n",
    "\n",
    "all_experiments_scores = all_experiments_scores.round(4)\n",
    "\n",
    "df_transposed = all_experiments_scores.transpose()\n",
    "\n",
    "# plot the bar chart\n",
    "ax = df_transposed.plot(kind='bar', figsize=(30, 10), width=0.9, fontsize=16)\n",
    "\n",
    "# set labels\n",
    "ax.set_title('Performance of experiments - average recent epochs', fontsize=22)\n",
    "ax.set_xlabel('Score Type')\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "# show the legend\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "# add numbers to bars\n",
    "for i, rect in enumerate(ax.containers):\n",
    "    ax.bar_label(rect, labels=all_experiments_scores.iloc[i].astype(str), fontsize=16)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, \"discriminator_compare_last_x.jpg\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
